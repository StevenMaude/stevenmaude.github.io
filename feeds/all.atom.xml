<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>stevenmaude.co.uk</title><link href="https://www.stevenmaude.co.uk/" rel="alternate"></link><link href="https://www.stevenmaude.co.uk/feeds/all.atom.xml" rel="self"></link><id>https://www.stevenmaude.co.uk/</id><updated>2022-09-07T21:11:00+01:00</updated><entry><title>Link checking with lychee</title><link href="https://www.stevenmaude.co.uk/posts/link-checking-with-lychee" rel="alternate"></link><published>2022-09-07T21:11:00+01:00</published><updated>2022-09-07T21:11:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2022-09-07:/posts/link-checking-with-lychee</id><summary type="html">&lt;p&gt;A rundown of validating URLs with lychee&lt;/p&gt;</summary><content type="html">&lt;h2 id="the-use-of-urls-in-writing"&gt;The use of URLs in writing&lt;/h2&gt;
&lt;p&gt;Lots of information these days is on the web. Here, you're looking at
some.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;And with that, this means that URLs, often in the form of links to web
content, are also heavily used in all kinds of writing: other web pages,
reports, papers, books.&lt;/p&gt;
&lt;p&gt;In some of those cases, writing is published and then largely "done",
unless there's a revised edition. Writing on the web is, ideally for the
reader, maintained, if that's appropriate. Even if you're not
maintaining the links after you've published something, you might want,
if you're writing over a long period of time, to at least assert that
the links you may have added sometime ago are still valid at
publication. Electronic formats at least offer this possibility, even if
that possibility is often unfulfilled.&lt;/p&gt;
&lt;p&gt;Writing often becomes less relevant with time — particularly in the
current climate of "content creation" (and it is difficult to see right
now how this might ever slow down). But some ideas do have a more
enduring relevance. Although, even if the ideas endure, the URLs linked
as part of that work may not. Often, an author may link to sources they
did not create and do not have any control over. If a resource you don't
control disappears, then, well, it disappears.&lt;/p&gt;
&lt;h2 id="validating-links"&gt;Validating links&lt;/h2&gt;
&lt;p&gt;So, let's assume what you've written is useful or relevant, at least, to
readers present and future. How can you validate that the links you have
in whatever documents you're dealing with are still functioning?&lt;/p&gt;
&lt;p&gt;You could hack together a shell script or similar that will do this and
give you a good idea of which links are broken. It's often less work to
choose existing software, should something suitable exist, for fairly
common tasks. The authors have then done the work for you, including
thinking about handling any awkward cases or behaviour.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/lycheeverse/lychee"&gt;lychee&lt;/a&gt; is one such program
designed to check URLs in collections of text-based source files. You
can also run it against a website, but I've found it most useful for
"local"&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; files.&lt;/p&gt;
&lt;p&gt;A lychee link check runs &lt;em&gt;very&lt;/em&gt; quickly. Each run gives you a summary of
how many links were checked, how many were successful and reasons for
failures. As a guide, the link checking step in a GitHub Actions lychee
run takes about 10 seconds for a site with about 450 URLs.&lt;/p&gt;
&lt;h3 id="whats-nice-about-lychee"&gt;What's nice about lychee&lt;/h3&gt;
&lt;p&gt;lychee is particularly useful in that it can check various plain text
formats: whether HTML, Markdown, reStructuredText, or other text files.
You can also point to an existing website and check the links there.&lt;/p&gt;
&lt;p&gt;On top of that, lychee is cross-platform. Since I have always run it via
GitHub's hosted virtual machine runners, it's perhaps less important how
it runs.&lt;/p&gt;
&lt;p&gt;As a quick summary, it's nice that it worked well from the start,
without spending lots of times debugging. The results I initially got
from it were, on the the whole, useful.&lt;/p&gt;
&lt;h2 id="possible-applications"&gt;Possible applications&lt;/h2&gt;
&lt;p&gt;There are more uses than you might first think of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;web content (the obvious use)&lt;/li&gt;
&lt;li&gt;"formal" documentation sources&lt;/li&gt;
&lt;li&gt;collections of less formal READMEs that act as documentation within a
  coding project&lt;/li&gt;
&lt;li&gt;a research paper or thesis, to check the links before submission&lt;/li&gt;
&lt;li&gt;anywhere else you have a collection of text files (applied to
  collections of notes, or digital
  &lt;a href="https://en.wikipedia.org/wiki/Zettelkasten"&gt;Zettelkästen&lt;/a&gt; or similar)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And checking links has the auxiliary benefit of checking your own
content: if the links that relate to a particular point are defunct, it
may be that the underlying information or situation has changed. That's
particularly the case with computing where information can date rapidly,
due to software or hardware developments or changes in best
practice.&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id="some-usage-notes"&gt;Some usage notes&lt;/h2&gt;
&lt;h3 id="within-github"&gt;Within GitHub&lt;/h3&gt;
&lt;p&gt;It is possible to run lychee within a GitHub Actions runner. There is an
existing &lt;a href="https://github.com/lycheeverse/lychee-action"&gt;action&lt;/a&gt; or you
could write your own workflow to download and run the latest version.&lt;/p&gt;
&lt;p&gt;This way, you can run lychee automatically, either on a schedule or on a
pull request. This works relatively well, with the additional benefit
that the link checking isn't being done using your own internet
connection.&lt;/p&gt;
&lt;p&gt;Other tips for running in GitHub Actions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For checking GitHub links, you may want to use &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; with
  lychee or the action which avoids rate limiting. You may want to
  restrict that token, to, for example, just &lt;em&gt;read&lt;/em&gt; the contents of the
  repository with the &lt;code&gt;permissions&lt;/code&gt; key.&lt;/li&gt;
&lt;li&gt;GitHub Actions runners get blocked by various sites, so you may have
  to exclude a few links. You can add a &lt;code&gt;.lycheeignore&lt;/code&gt; file which
  allows you to specify links via regular expressions.&lt;/li&gt;
&lt;li&gt;You cannot check email addresses within a GitHub-hosted runner, so you
  need to use the &lt;code&gt;--exclude-mail&lt;/code&gt; option.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="outside-of-github"&gt;Outside of GitHub&lt;/h3&gt;
&lt;p&gt;Restrictions outside of GitHub might be less onerous, but at the risk of
getting your IP address rate limited or even temporarily blocked. So I'd
be very wary of running something like lychee using a personal internet
connection.&lt;/p&gt;
&lt;p&gt;If you have access to cloud virtual machines, that might be another
option. However, the IP address ranges that such a virtual machine might
have could already be blocked by the sites (or their hosting providers)
that you're trying to check.&lt;/p&gt;
&lt;p&gt;There are two things you could do to be careful:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set a maximum number of concurrent connections.&lt;/li&gt;
&lt;li&gt;Run the &lt;code&gt;--dump&lt;/code&gt; option just to see which links would be checked; if
  there are any in there which are important for you to access for and
  might rate limit or block you, maybe consider adding them to the
  &lt;code&gt;.lycheeignore&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="wherever-youre-running"&gt;Wherever you're running&lt;/h3&gt;
&lt;p&gt;Basic authentication is supported for logging into sites, but links that
require some kind of two-factor authentication can't be checked. Whether
you want to go to the trouble of creating an account, particularly if
that account's credentials are solely for GitHub use, just for checking
a few links is another question.&lt;/p&gt;
&lt;h3 id="integrating-into-a-documents-build-process"&gt;Integrating into a document's build process&lt;/h3&gt;
&lt;h4 id="a-rough-strategy"&gt;A rough strategy&lt;/h4&gt;
&lt;p&gt;Initially, particularly for a collection of files that may have not been
written or reviewed recently, it's likely that there are several broken
links.&lt;/p&gt;
&lt;p&gt;This is the approach I've taken:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run lychee.&lt;/li&gt;
&lt;li&gt;Review the error log.&lt;/li&gt;
&lt;li&gt;Check and classify the failures.&lt;/li&gt;
&lt;li&gt;Decide whether to fix up links, or ignore them by adding to a
   &lt;code&gt;.lycheeignore&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;Repeat the previous steps until you have no errors.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can see a real-world example of this &lt;a href="https://github.com/opensafely/documentation/issues/642"&gt;that I used at
work&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id="resolving-failures"&gt;Resolving failures&lt;/h4&gt;
&lt;p&gt;Failures tended to fall into one of these categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Genuinely broken links.&lt;/li&gt;
&lt;li&gt;Links to resources that are restricted to authorised users: you need
  to login first.&lt;/li&gt;
&lt;li&gt;Links that were inaccessible from within the GitHub Actions runner
  virtual machines.&lt;/li&gt;
&lt;li&gt;Internal Markdown or HTML template links that were not getting
  interpreted correctly, and, in some cases, could be corrected.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My approach has been to skim through the errors by hand and classify
them. You can also get JSON output if it's useful to have a directly
machine-readable format.&lt;/p&gt;
&lt;p&gt;From the errors and your investigation, you can create a &lt;code&gt;.lycheeignore&lt;/code&gt;
file: an exclusion list of links. That file is used if it is in the
working directory, or you can specify its path explicitly via lychee's
options. Adding a &lt;code&gt;.lycheeignore&lt;/code&gt; file at the top-level of your source
repository, if you are using a version control repository, works well.&lt;/p&gt;
&lt;p&gt;You can specify exclusions by regular expressions: excluding subdomains
works quite well when there are multiple links to that subdomain. You
can just include the direct URL if there's just one specific URL for a
specific domain you want to exclude. It's worth generalising the
exclusion once you get more variations of a single URL or domain.&lt;/p&gt;
&lt;p&gt;For fixing up broken links, then either you need to find where the
relevant URL has moved to, if it is still available, or review whether
the &lt;a href="https://web.archive.org"&gt;Internet Archive&lt;/a&gt; has a copy saved. If
it's for a blog post, you might want to pick a version that's close in
date to when you added the link to that post: that version is possibly
what you'll have been reading. Alternatively, you could review the most
recent available version and use that version if suitable.&lt;/p&gt;
&lt;p&gt;For the couple of projects I've tried lychee with, it's able to check
something like 80-90% of URLs. The checks have been useful and have
flagged URLs and surrounding text that require updating. This will vary
depending on where the URLs you are checking point to though.&lt;/p&gt;
&lt;h3 id="miscellaneous-tips"&gt;Miscellaneous tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;If you are particularly finicky, it is possible to flag redirects by
  setting the number of maximum redirects to 0. And it is possible to
  check for non-HTTPS web URLs: these days most URLs should be available
  via HTTPS.&lt;/li&gt;
&lt;li&gt;You can set another User-Agent. I've not tried this to see if it
  improves the accessibility of some URLs.&lt;/li&gt;
&lt;li&gt;Excluding directories is, at time of writing, clunky; see the
  &lt;a href="https://github.com/lycheeverse/lychee/issues/470"&gt;associated GitHub
  issue&lt;/a&gt;. This is the
  only finicky usability hurdle I've encountered.&lt;/li&gt;
&lt;li&gt;If your documents are not in a supported format, lychee is still
  potentially useful. You would need to export your document to a plain
  text format, either in whatever authoring tool you are using, or with
  something like &lt;a href="https://pandoc.org"&gt;pandoc&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;There are lots of other options that you can use with lychee: take a
  look at the documentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="persistence-of-web-content"&gt;Persistence of web content&lt;/h2&gt;
&lt;p&gt;So, you can probably tell: I think lychee is a really useful tool.&lt;/p&gt;
&lt;p&gt;However, its usefulness comes with a caveat: &lt;strong&gt;all lychee can do is tell
you whether a link is accessible or not&lt;/strong&gt;. But, wait, you might argue:
that's the whole point of this post that I'm reading — we &lt;em&gt;want&lt;/em&gt; to
check links, right?&lt;/p&gt;
&lt;p&gt;In a way, yes.&lt;/p&gt;
&lt;p&gt;All a valid link tells you is that the relevant resource provider has
some resource available and accessible at that URL.&lt;/p&gt;
&lt;p&gt;It &lt;strong&gt;doesn't&lt;/strong&gt; assure you that whatever you've linked to is in anything
like the state it was when you read it. There are no guarantees that
typical web URLs point to static or immutable content. They work more
like pointers: it's up to the relevant server hosting the URL's domain
and the people who put content on that server that decide what the
content is.&lt;/p&gt;
&lt;p&gt;In many cases, this could be fine. In others, maybe less so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The original author may have updated their content in a way that might
  change the relevance to whatever argument or citation you intended to
  make.&lt;/li&gt;
&lt;li&gt;Another organisation could have taken over (perhaps legitimately,
  perhaps not) the site and decided to post a contrary view, or
  something entirely different at the same URL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Making HTTP requests and checking for a non-error response codes only
tells you that a resource is available at that URL, whether directly or
with redirection somewhere else. It doesn't guarantee that a resource is
actually as you read it, when you read it.&lt;/p&gt;
&lt;h3 id="preservation"&gt;Preservation&lt;/h3&gt;
&lt;p&gt;You may now be thinking: well, I could just take a copy and link to
that, providing my own cache of the resource as it was when I reviewed
it. That would work, but hosting that copy is probably a bad idea:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are potential copyright issues.&lt;/li&gt;
&lt;li&gt;If you're intending to host this copy on the web, then, if it is
  public and discoverable, it's possible that search engine providers
  think you're up to some kind of nefarious search engine optimisation
  shenanigans.&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Much simpler is linking to the &lt;a href="https://web.archive.org"&gt;Internet
Archive&lt;/a&gt;: either finding a "good" copy there
already, or using their page save feature to archive a copy of the page
there and then.&lt;/p&gt;
&lt;p&gt;Wait, so why wouldn't we just always link to the Internet Archive?&lt;/p&gt;
&lt;p&gt;In some limited cases, this might be useful, where you want readers to
have the information as you accessed it at the time. That is
particularly useful if you are specifically citing a work and the
version available at a given time. In many other cases, it is still
useful to link to the "live" resource because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You may be pointing to an interactive service, and the Internet
  Archive's more static version may not function correctly, without
  access to the original host's backend servers.&lt;/li&gt;
&lt;li&gt;You may want your readers to always access the latest published
  version of the information that you link.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we link to the original host instead of a third-party preservation
service, this does pose the future risk of the resource being
unavailable in future. But tools like lychee can definitely help with
maintaining working URLs.&lt;/p&gt;
&lt;p&gt;And, finally, if you're now asking: "what guarantees are there on
availability and integrity of the content the Internet Archive hosts".
Or maybe more succinctly: "who archives the archivers?" Let's say that
those are also good questions, but there are already enough words here
not to worry about this here and now.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Unless you've tried to subvert that by printing this out. You
  rascal.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;The quote marks are because all my running of lychee has been via
  GitHub Actions: a "local" checkout of my source files on a remote
  virtual machine.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;Indeed, Stack Overflow are finding this a problem. It turns out
  that sifting through years of accumulated answers, some now
  deprecated, is a &lt;a href="https://meta.stackoverflow.com/questions/405302/introducing-outdated-answers-project"&gt;big spring cleaning
  task&lt;/a&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;That said, there is less evidence from Google's search results
  that this rule is actually &lt;em&gt;enforced&lt;/em&gt;. Sites that are obviously
  cloning contents from other sites do sometimes rank quite highly.&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="2022"></category><category term="blog"></category><category term="documentation"></category><category term="documentation as code"></category><category term="link"></category><category term="lychee"></category><category term="URL"></category><category term="research software engineering"></category><category term="writing"></category></entry><entry><title>PyCon 2021: Secure software supply chains</title><link href="https://www.stevenmaude.co.uk/posts/pycon-2021-secure-software-supply-chains" rel="alternate"></link><published>2021-10-15T17:27:00+01:00</published><updated>2021-10-15T17:27:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2021-10-15:/posts/pycon-2021-secure-software-supply-chains</id><summary type="html">&lt;p&gt;A summary of a talk by Dustin Ingram at PyCon 2021.&lt;/p&gt;</summary><content type="html">&lt;h2 id="modern-python-less-modern-me"&gt;Modern Python, less modern me&lt;/h2&gt;
&lt;p&gt;I've been working for the past few months at The DataLab — now Bennett
Institute for Applied Data Science. There, Python is the main language
for development and collectively the developers have a lot of Python
experience.&lt;/p&gt;
&lt;p&gt;Because of that, I've seen a lot more modern Python than I was
accustomed to seeing or writing. What I've found is what I knew about
Python is a little outdated. Certainly there have been a lot of new
Python features introduced that I didn't know about or only vaguely had
heard of, e.g.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dataclasses &lt;/li&gt;
&lt;li&gt;assignment expressions, i.e. &lt;code&gt;:=&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;breakpoint()&lt;/code&gt; instead of having to import and use the &lt;code&gt;pdb&lt;/code&gt; package
  directly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It seemed wise to me then to watch some recent conference talks. I may
write up notes on them. All that said, let's look at the first one.&lt;/p&gt;
&lt;h2 id="secure-software-supply-chains-for-python"&gt;Secure Software Supply Chains for Python&lt;/h2&gt;
&lt;p&gt;This is a summary of &lt;a href="https://www.youtube.com/watch?v=VWWgkF-0cDQ"&gt;this
talk&lt;/a&gt; by Dustin Ingram, a
PyPI maintainer.&lt;/p&gt;
&lt;p&gt;The talk describes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What software supply chain attacks are.&lt;/li&gt;
&lt;li&gt;The current best Python practice for developes.&lt;/li&gt;
&lt;li&gt;What improvements could be made to &lt;code&gt;pip&lt;/code&gt; and PyPI in future.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="supply-chain-attacks"&gt;Supply chain attacks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;They are a thing.&lt;ul&gt;
&lt;li&gt;Involve a compromise of your dependencies, where they are
  distributed and everything used to build them.&lt;/li&gt;
&lt;li&gt;This applies recursively to your dependencies' dependencies and so
  on.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="types-of-supply-chain-attack"&gt;Types of supply chain attack&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;MITM attack: intercepting a package download and replacing it with
  something malicious.&lt;/li&gt;
&lt;li&gt;Typosquatting: uploading malicious packages with names similar to
  well-known packages.&lt;/li&gt;
&lt;li&gt;Dependency confusion: where internal, non-public packages are used,
  but &lt;code&gt;pip&lt;/code&gt; is misconfigured to check PyPI first and someone registers a
  malicious public package with the same name as the internal package.&lt;/li&gt;
&lt;li&gt;"Research": people deliberately introducing vulnerabilities into code.&lt;/li&gt;
&lt;li&gt;SolarWinds was mentioned as an example: compromise of build system.&lt;/li&gt;
&lt;li&gt;Compromised maintainers.&lt;/li&gt;
&lt;li&gt;Compromised source control.&lt;/li&gt;
&lt;li&gt;Leaked passwords/API tokens.&lt;/li&gt;
&lt;li&gt;Social engineering.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="python-best-practices"&gt;Python best practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;For Python, best current practices are:&lt;ul&gt;
&lt;li&gt;Use pinned requirements and with hashes, e.g. via &lt;code&gt;pipenv lock&lt;/code&gt;,
  or &lt;code&gt;pip-compile --generate-hashes&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Use some notification service that monitors dependencies in your
  source, e.g. Dependabot (this is convenient, but is an extra
  component in the supply chain: you have to trust that Dependabot
  isn't creating malicious pull requests).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;There are other possible future improvements to PyPI and &lt;code&gt;pip&lt;/code&gt;:&lt;ul&gt;
&lt;li&gt;Removal of &lt;code&gt;setup.py&lt;/code&gt; support to stop arbitrary code being run at
  install time. That might save you if you realise you've installed
  a compromised package, before importing and using the package.&lt;/li&gt;
&lt;li&gt;Signed repository metadata (&lt;a href="https://www.python.org/dev/peps/pep-0458/"&gt;PEP 458&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Having namespaces on PyPI.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="2021"></category><category term="PyCon"></category><category term="Python"></category></entry><entry><title>A review of "UNIX: A History and a Memoir"</title><link href="https://www.stevenmaude.co.uk/posts/a-review-of-unix-a-history-and-a-memoir" rel="alternate"></link><published>2021-05-08T00:12:00+01:00</published><updated>2021-05-08T12:16:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2021-05-08:/posts/a-review-of-unix-a-history-and-a-memoir</id><summary type="html">&lt;p&gt;A quick recommendation of an easy-to-read and brisk summary of the development of UNIX.&lt;/p&gt;</summary><content type="html">&lt;p&gt;"UNIX: A History and a Memoir" is from the perspective of Brian
Kernighan, who was around at the inception of UNIX, and is well-known in
computing himself as one of the three authors of the AWK language and
the author of a number of other influential books (e.g. "The C
Programming Language" by Kernighan and Ritchie). The title covers most
of what the book deals with: the formative years of UNIX, with
explanation of the key software that was developed as part of it.&lt;/p&gt;
&lt;p&gt;It explains where UNIX was developed and who did that work. Initially,
UNIX was one of Ken Thompson's side projects, quickly capturing wider
interest and sparking collaboration within Bell Labs. This was all amid
a milieu of Bell Labs having considerable operating system development
experience as a collective, yet having no operating system to work on.
(Bell Labs had been working on the Multics project, but eventually
withdrew its support.) Eventually, management managed to gather funding
for UNIX, initially with a view to improving how patent applications
within Bell Labs were prepared.&lt;/p&gt;
&lt;p&gt;From there, UNIX developed sufficiently that it found wider use within
Bell Labs, within AT&amp;amp;T — Bell Labs' then parent company — and within
many universities. Eventually UNIX was sold commercially. Later, while
various commercial UNIX vendors expended much efforts in commercial
wrangling of various parties, the appearance of the free Linux kernel in
the early 1990s paved the way for Linux distributions to render the
commercial squabbles largely obsolete.&lt;/p&gt;
&lt;p&gt;Kernighan's rundown of the features that made UNIX distinct from many of
the competitors at the time explains why UNIX was a key development that
transformed computing at the time, offering:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a hierarchical file system;&lt;/li&gt;
&lt;li&gt;accessible system calls;&lt;/li&gt;
&lt;li&gt;abstraction of other concepts, such as hardware devices, that they
  could be worked with much as regular files;&lt;/li&gt;
&lt;li&gt;a command-line shell that could be used composably, along with a suite
  of programs as useful tools, pipes and scripting to connect those
  programs together to effectively create new commands or programs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Half a century on and these features are still evident in Linux and
other descendants that are still being developed today.&lt;/p&gt;
&lt;p&gt;Kernighan closes the book by describing factors that made Bell Labs such
a successful research institution. That in itself is an interesting
story that entwines with the development of UNIX. My distillation is
that Bell Labs had an extended period where the organisation:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;had interesting and challenging problems to solve;&lt;/li&gt;
&lt;li&gt;hired very talented staff to solve those problems;&lt;/li&gt;
&lt;li&gt;had sufficient resources that the staff did not need to worry about
  funding, while management could take a longer view on projects without
  demanding immediate results;&lt;/li&gt;
&lt;li&gt;had a technically competent management;&lt;/li&gt;
&lt;li&gt;offered a collaborative and fun environment for this research.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All easier written than done, of course. If it were so simple, that
research model would be simply replicated everywhere given the
resources. Nonetheless, this goes some way to summarise why Bell Labs
were so extraordinarily influential and tremendously prolific in 20th
century technology developments.&lt;/p&gt;
&lt;p&gt;Finally, the book also puts the development of UNIX into context in
modern-day computing. Many computing devices today run operating systems
that have some connection back to UNIX. Even Microsoft has adopted
several of the ideas made popular by UNIX, and has, at times, veered
closer to UNIX and its descendants, offering Windows Services for UNIX
in the past, Windows Subsystem for Linux in the present and, long ago,
even distributing XENIX, Microsoft's very own UNIX distribution! While
UNIX is over 50 years old, it continues to influence today's computing:
this well-written book certainly helps to understand why that is so.&lt;/p&gt;</content><category term="2021"></category><category term="book"></category><category term="computing"></category><category term="history"></category><category term="Linux"></category><category term="review"></category><category term="UNIX"></category></entry><entry><title>Tracking down tracks</title><link href="https://www.stevenmaude.co.uk/posts/tracking-down-tracks" rel="alternate"></link><published>2021-04-24T22:45:00+01:00</published><updated>2021-04-24T22:45:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2021-04-24:/posts/tracking-down-tracks</id><summary type="html">&lt;p&gt;Strategies for finding details on elusive pieces of music.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Having recently gone through an extensive process — well, thirty minutes
or so of searching — to find a piece of music, I wanted to document the
approaches I know of for this process.&lt;/p&gt;
&lt;p&gt;Here, for "music", I'm primarily thinking about relatively modern
Western dance music from the 1970s to the present day — disco to
dubstep, and beyond. Some ideas detailed here may well apply to other
genres and time periods.&lt;/p&gt;
&lt;h2 id="take-a-recording"&gt;Take a recording&lt;/h2&gt;
&lt;p&gt;What if you're listening to something right now and want to track it
down? Record it. There's a lot to be said about the ubiquity of phones.
Here though, having a portable recording device in your pocket is really
useful. Someone you want to ask or some tool you want to use will have a
much easier time with a real recording, even if imperfect, over merely a
description or your best guess at humming or singing the music.&lt;/p&gt;
&lt;p&gt;There are several music recognition applications that are very effective
and well-known these days. These typically work by submitting an audio
fingerprint to the application owner's servers and searching for that in
their databases. I don't personally use these applications, though I
have tried them occasionally in the past.&lt;/p&gt;
&lt;p&gt;It is possible to use these applications live while the music is
playing. But you still might favour taking a recording first so that you
can easily share that elsewhere. Audio recognition applications may not
cover every piece of music ever recorded, but they are potentially a
useful first look. As these applications often run on mobile devices,
there is a privacy-related caveat&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;, however.&lt;/p&gt;
&lt;p&gt;With an audio clip of interest — whether by recording it yourself, or
snipping it from an existing piece of digital audio — an alternative is
to upload the audio to a site that detects content, e.g. SoundCloud,
Mixcloud, YouTube etc. Primarily the reason is for these services to
detect copyrighted content and either block your submission, or at least
monetise it for the copyright owner. But you can repurpose this to
identify the audio for you.&lt;/p&gt;
&lt;p&gt;This is also useful if you want to identify lots of tracks from one
source, e.g. a DJ mix, in one go. Often, I've listened to things on
YouTube and spotted the video description details several of the tracks
in there.&lt;/p&gt;
&lt;p&gt;And what to do if recording isn't possible, the recording fails or is
not clear enough to be useful? If there are lyrics or a vocal sample,
then remembering the most frequent lyric and/or distinctive lyrics is
helpful. A frequently repeated lyric, or a modification of it, may well
be the music's title. Lyric fragments are easily searched for.&lt;/p&gt;
&lt;h2 id="narrowing-down"&gt;Narrowing down&lt;/h2&gt;
&lt;p&gt;Searching for the text used in lyrics or repeated vocal samples can be a
good start. Because vocals are often sampled from elsewhere, you may end
up first locating the original sample source. If you do, that's a lead:
you can next try and search for which tracks sampled that vocal.
Especially in early 90s tracks, certain acapellas that were used many,
many times.&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;You can also search for the name of the genre, along with the lyric
snippet. This might also help get you to some place on the web. That
might be where the release is indexed in some kind of database, e.g.
Discogs. It might occasionally be some place where someone else is
asking the same question as you, possibly with a helpful reply.&lt;/p&gt;
&lt;p&gt;Try restricting a search to some of the bigger music-related databases
online. You can try using the &lt;code&gt;site:&lt;/code&gt; operator on search engines that
support it to narrow your search. Looking for a combination of the
artist and name of original track, optionally with a genre on Discogs,
&lt;code&gt;site:discogs.com&lt;/code&gt;, might help.&lt;/p&gt;
&lt;p&gt;If it's a mix with a dated year, you can probably narrow down your
search to a year or two; the release may well be from that same year or
previous years. It does roughly narrow it down, but not precisely. If
the recording is from towards the end of a year, the release might have
been the next year (or even later) if it has been shared with DJs ahead
of release. If there are commercial issues, e.g. trouble with sample
clearance or labels, it might never have been officially released, or
may have been released considerably later.&lt;/p&gt;
&lt;p&gt;If it's a genre you know well and particularly for Western electronic
dance music, especially historically, you can probably have some kind of
guess as to when a piece of music was produced to the nearest couple of
years. Even for today's dance music, where there might not be the same
huge difference in recording techniques as there was from the 1980s to
2000s, there may certainly be trends, common production styles or
popular sounds or samples. That said, there's also a fashion for making
things sound like they used to — producers can use the same techniques
and equipment that were popular during a certain period to replicate it
— so don't always assume your guess is correct.&lt;/p&gt;
&lt;p&gt;Another approach that might work if others don't and the track was in a
DJ set is finding more sets by the same DJ from around the same time
period. Narrow down by looking for sets with tracklistings and then skim
through to see if the same track appears. This requires some luck, but
might just work.&lt;/p&gt;
&lt;h2 id="getting-help-from-others"&gt;Getting help from others&lt;/h2&gt;
&lt;p&gt;If you hear the music at an event you're actually attending and a DJ is
playing, or played the track, well, you can try asking them either at
the event, or after the fact online. This is more likely to be
successful at small events. You can always try this if it's a bigger
show or the DJ in question is a bigger name, though that may be less
successful.&lt;/p&gt;
&lt;p&gt;If it's a recorded DJ mix, there may well be a tracklisting somewhere.
That goes especially if it's a commercial mix, or if it was a radio
broadcast (where stations often list show details on their websites).&lt;/p&gt;
&lt;p&gt;If the mix is on SoundCloud or YouTube, you can look at the description
or the comments. Sometimes, particularly on SoundCloud, you might get
the DJ themselves, the artist or someone else who follows that DJ or
genre naming some or all of the tracks.&lt;/p&gt;
&lt;p&gt;If all that fails, you could always try asking somewhere online. If it's
a mix that's already posted online, you could ask in the comments of
that mix. If it's audio that you've recorded elsewhere, you can upload
it somewhere, or describe it as best you can. Find a suitable place to
then post that question. That could be somewhere more general, like the
Discogs forums, or more specific where certain music genres are the
focus.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;The caveat is that there are possible privacy issues with using
  these types of applications. These applications may request
  permissions to record audio, as well as locating you via GPS or to a
  lesser extent, IP address. Incidentally, Shazam were bought by Apple,
  and Apple seem to place some priority on user privacy. Having not used
  Shazam much, if at all, I'm not necessarily advocating it though.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;A good question, that I can't answer right now, is &lt;em&gt;why&lt;/em&gt; certain
  vocals were reused so much. Was it because of the availability of the
  original acapella? Was it because the vocal has a particular sound?
  Was it a deliberate reference to the existing popular reuse?&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="2021"></category><category term="DJ"></category><category term="music"></category></entry><entry><title>Things I have learned from rebuilding a PC in 2021</title><link href="https://www.stevenmaude.co.uk/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021" rel="alternate"></link><published>2021-02-26T14:02:00+00:00</published><updated>2021-02-26T14:02:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2021-02-26:/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021</id><summary type="html">&lt;p&gt;What are you buying? What are you selling?&lt;/p&gt;</summary><content type="html">&lt;p&gt;Despite working with computers (supposedly) professionally, I don't
often build PCs or upgrade PC hardware. A few years ago, I learned a few
lessons from &lt;a href="https://www.stevenmaude.co.uk/posts/things-ive-learned-from-building-and"&gt;building a PC&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Over the 2020 Christmas holidays, I was upgrading someone's PC for them,
so here are my lessons I've thought about in 2021.&lt;/p&gt;
&lt;h2 id="buying-pc-parts-is-not-easy"&gt;Buying PC parts is &lt;em&gt;not&lt;/em&gt; easy&lt;/h2&gt;
&lt;p&gt;In the UK at least, the latest processors (CPUs) and graphics cards
(GPUs) were out of stock at the time of buying. That is probably due to
some or all of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;COVID-19 related supply chain issues;&lt;/li&gt;
&lt;li&gt;releases of hardware in the run up to the holiday period;&lt;/li&gt;
&lt;li&gt;the current trend for people to buy coveted, in-demand trinkets
  (fashion, new generation video game consoles, hand sanitiser) and
  resell at higher than retail price;&lt;/li&gt;
&lt;li&gt;demand from people stuck at home and spending money on PC hardware
  instead of other things.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Adding to all that, the 2021 changes in UK trade due to Brexit may not
help either. However, the fundamental issue of shortages applies
globally from what I've read.&lt;/p&gt;
&lt;p&gt;Pent-up demand then drove up prices of older kit. By chance, I managed
to spot AMD's new Ryzen 5 5600x CPUs available just before Christmas
from a retailer at slightly higher than the billed retail price. Somehow
I also managed to order while holding my nose at the inflated cost. I
wasn't actually intending to buy the latest CPU, but the previous
generation Ryzen 5 3600x was being priced at around 75-80% of the cost
of the newer kit anyway.&lt;/p&gt;
&lt;p&gt;This might all resolve itself in time. I could do with a new desktop
build myself, but not really inclined to do so right now. Reading
around, it seems like stock shortages, particularly for GPUs, will
continuing well into 2021.&lt;/p&gt;
&lt;h2 id="buying-from-a-retailer-with-good-returns-policies-can-save-time"&gt;Buying from a retailer with good returns policies can save time&lt;/h2&gt;
&lt;p&gt;Here's a long and dull story; feel free to skip to the lessons below.
As part of the same build, and after a lot of research, I ordered a
mainboard that was on offer, as researching it showed it to be a good
buy. As lots of mainboards are, it seemed to be one with lots of RGB
LEDs — nice if you want that, but unnecessary to me — and I figured you
could disable the LEDs easily.&lt;/p&gt;
&lt;p&gt;Shortly after ordering, I discovered that you couldn't simply toggle the
LEDs off in the BIOS. Instead, you had to have the mainboard's
manufacturer's clunky software — nice if you want that, but unnecessary
to me — running constantly on Windows to control the LEDs.&lt;/p&gt;
&lt;p&gt;I tried to cancel the board order. The order hadn't yet moved to the
stage of being prepared for shipping and was temporarily out of stock
anyway. So I sent a message via the website and hoped they would cancel
on the next working day. This was almost out of office hours, and I
figured that everything would be OK.&lt;/p&gt;
&lt;p&gt;About an hour later, I got another email saying the order was
dispatched. From there, things became more of a mess. The next morning,
the courier shipping the order sent me a message with delivery options
including delaying the delivery. I delayed it, the courier still arrived
with the parcel and I explained that I'd deferred delivery.&lt;/p&gt;
&lt;p&gt;After a phone call to the company I ordered from, confirming I could
refuse delivery and they would create an returns number, I waited a
week, and heard nothing at all. There was no repeat delivery and I
assumed the parcel was in limbo somewhere.&lt;/p&gt;
&lt;p&gt;In the end, it took over a week for the company to get the parcel back
from the courier, it then took a further two written requests to
customer support to finally receive my money back.&lt;/p&gt;
&lt;p&gt;I told you it was a long and dull story. No refunds.&lt;/p&gt;
&lt;h3 id="a-long-and-dull-story-with-two-lessons-learned"&gt;A long and dull story with two lessons learned&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Ordering from a company that has an automated online system for
  cancelling orders is useful for sudden onset of buyer's remorse.&lt;/li&gt;
&lt;li&gt;Ordering from a company that has a good and simple returns process is
  useful if you find the hardware isn't right after you've received it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is one obvious market-leading online retailer that has these
attributes, but may not be everyone's first choice for various reasons.
Nonetheless, on customer service, these attributes are fantastic from a
buyer's perspective.&lt;/p&gt;
&lt;p&gt;I don't frequently make regrettable purchases. But you can do all the
reading of specifications, manuals, reviews and opinions that you like,
and still find two PC components just will not get along for whatever
reason.&lt;/p&gt;
&lt;p&gt;Having no-fuss returns can save you a lot of time and frustration
chasing up customer support. In the UK, there is &lt;a href="https://www.legislation.gov.uk/ukpga/2015/15/contents"&gt;substantial current
legislation regarding consumer
rights&lt;/a&gt;. However,
it's far simpler as a customer if the vendor already provides better
than the minimum customer service.&lt;/p&gt;
&lt;h2 id="selling-pc-parts-is-easy"&gt;Selling PC parts &lt;em&gt;is&lt;/em&gt; easy…&lt;/h2&gt;
&lt;p&gt;After the upgrade, we advertised the old removed parts online. They all
sold within a week, and the proceeds could be spent on a new graphics
card, if you could buy one (see above). I was surprised at both how
quickly these old components sold and for what we sold them for. Some
parts sold for not much less than they cost at retail. It's possible
that people are looking for budget builds, or upgrading/repairing
existing builds.&lt;/p&gt;
&lt;h3 id="but-make-sure-you-upgrade-any-firmware-before-selling"&gt;…but make sure you upgrade any firmware before selling&lt;/h3&gt;
&lt;p&gt;After the mainboard sold, the buyer asked about what CPU was used with
it; it wasn't working for them and they were trying to diagnose it. When
they booted up the PC, nothing happened.&lt;/p&gt;
&lt;p&gt;Between the details the buyer gave and reading around the BIOS release
notes for the mainboard, we concluded that it was probably a combination
of the mainboard BIOS never being updated, and the buyer using a newer
CPU than had previously been installed.&lt;/p&gt;
&lt;p&gt;It was lucky that there was a route to flashing the BIOS without a CPU
via one of the USB ports. Decent mainboards often feature this recovery
option. That meant the buyer could solve this problem without borrowing
or buying another CPU to upgrade the BIOS, or, worse for us, returning
the item.&lt;/p&gt;
&lt;p&gt;The wise thing to do is to upgrade the firmware, if there is any, before
removing components.&lt;/p&gt;
&lt;h2 id="what-tedious-cpu-cooler-installations-should-tell-hardware-manufacturers-about-marketing"&gt;What tedious CPU cooler installations should tell hardware manufacturers about marketing&lt;/h2&gt;
&lt;p&gt;Two very disparate ideas in the heading there; let's see if there's
anything like a cohesive argument here.&lt;/p&gt;
&lt;p&gt;Last time I wrote about &lt;a href="https://www.stevenmaude.co.uk/posts/things-ive-learned-from-building-and"&gt;building a
PC&lt;/a&gt;, I had
lots of fun installing Intel's push pins. This time I had fun with
fitting a cooler to an AMD setup. It didn't use screws. Instead the
cooler used long metal tweezer-like clips to fit over the tabs on the
AM4 socket on the mainboard.&lt;/p&gt;
&lt;p&gt;How this all was assembled to secure the cooler was not obvious at all.
The lack of clear instructions did not help one bit. The text basically
said "build the thing and look at the diagram". The diagram wasn't
clear.&lt;/p&gt;
&lt;p&gt;Nor were there many decent YouTube tutorials on this. Eventually I
managed to piece together what to do from looking collectively at
several installation videos in several different languages. Even then,
it was a delicate task to keep the assembly together, while placing the
cooler contact directly on the CPU package and not smudging thermal
paste everywhere.&lt;/p&gt;
&lt;p&gt;As an esteemed non-influencer, hardware manufacturers are doing whatever
the opposite is of queuing up to ask me what I think. But if there ever
did suddenly appear an itinerant queue of manufacturers on my doorstep,
what I'd advise them is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of relying on buyers of the hardware to provide tutorials,
  manufacturers should create or commission their own videos.&lt;/li&gt;
&lt;li&gt;Make sure these videos have appropriate metadata (title and
  description) to make them easy to find by those who need them.&lt;/li&gt;
&lt;li&gt;Provide videos in multiple languages, whether by audio, subtitles or
  both.&lt;/li&gt;
&lt;li&gt;Show less obvious steps from multiple angles.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It's probably justifiable to spend a relatively small portion of a
marketing and/or support budget to ensure that there is a decent
installation video available. This is marketing and brand awareness:
you're demonstrating, hopefully, that your kit is easy to use, and,
importantly, providing support for users.&lt;/p&gt;
&lt;p&gt;I think it's a wider lesson for anyone selling anything — whether
hardware or software — that might require some tricky user installation.
Whether a prospective buyer seeing how to install something and deciding
to buy based on that, or an actual buyer not complaining online or
requesting a refund, there's a benefit for the vendor too.&lt;/p&gt;</content><category term="2021"></category><category term="PC"></category><category term="build"></category></entry><entry><title>Copying large iPhone videos to a Windows PC</title><link href="https://www.stevenmaude.co.uk/posts/copying-large-iphone-videos-to-a-windows-pc" rel="alternate"></link><published>2021-01-25T20:50:00+00:00</published><updated>2021-01-25T20:50:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2021-01-25:/posts/copying-large-iphone-videos-to-a-windows-pc</id><summary type="html">&lt;p&gt;How to copy iPhone videos that don't appear in Windows File Explorer&lt;/p&gt;</summary><content type="html">&lt;h2 id="a-simple-task-made-complicated"&gt;A simple task made complicated&lt;/h2&gt;
&lt;p&gt;This isn't a particularly interesting post. That is, unless you reading
this now, like me at the time of writing this, have spent the best part
of an hour trying to solve the same issue. Like a few other posts I've
made in the past, the purpose is really to make this easier to search
for, if others encounter the same problem.&lt;/p&gt;
&lt;p&gt;Someone had recorded a video on an iPhone that they wanted to transfer
to a Windows PC. Naturally, they connected the phone via a USB cable to
the computer, navigated through the existing &lt;code&gt;DCIM&lt;/code&gt; directories to copy
the files across and found, well, nothing. So they asked me about it.&lt;/p&gt;
&lt;p&gt;What was going on? They used the same process before and it worked fine.
And the videos were definitely still on the phone: you could play them
there. As these videos had large file sizes, I suspected there was some
limit being encountered.&lt;/p&gt;
&lt;h2 id="no-easy-answers"&gt;No easy answers&lt;/h2&gt;
&lt;p&gt;Many of the workarounds posted online seem to fit one of the following
categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Moving the video via iCloud, or some other uploading route. (Too slow
  and too tedious.)&lt;/li&gt;
&lt;li&gt;Using some other app to get the video to a PC. (May involve spending
  money and requires auditing the app to ensure it and the developers
  are reputable.)&lt;/li&gt;
&lt;li&gt;Re-encoding the video on the phone to a smaller size somehow, possibly
  via iMovie? (Didn't try this and may have been slow. This would have
  been the next thing I suggested if the solution below had not worked.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Apple documentation itself suggests making sure iTunes is installed,
using the Windows Photos app and then importing from there. This didn't
work.&lt;/p&gt;
&lt;h3 id="the-fix"&gt;The fix&lt;/h3&gt;
&lt;p&gt;I found someone posted a fix on a forum. On the iPhone's Settings,
select Photos, and then under "Transfer to Mac or PC" setting for the
Photos app, change "Automatic" to "Keep Originals". That was it. Much
simpler than any of the other suggestions.&lt;/p&gt;
&lt;p&gt;The import actually worked through the Photos app then. In fact, you did
not need Photos at all: checking the &lt;code&gt;DCIM&lt;/code&gt; directory listed the file.&lt;/p&gt;
&lt;p&gt;What "Keep Originals" does is take a direct copy of the photos and
videos, instead of doing some unspecified magic and converting them on
copy.&lt;/p&gt;
&lt;p&gt;(Though not tested, iPadOS has the same setting and presumably behaves
in the same way.)&lt;/p&gt;
&lt;p&gt;As someone unfamiliar with iOS, I'm not entirely sure how you would
connect the setting to the behaviour. But the main thing is that it
resolved a very frustrating and very badly documented issue very, very
quickly.&lt;/p&gt;</content><category term="2021"></category><category term="iOS"></category><category term="iPad"></category><category term="iPadOS"></category><category term="iPhone"></category><category term="Windows"></category></entry><entry><title>Garmin Forerunner 35: a competent budget running watch</title><link href="https://www.stevenmaude.co.uk/posts/garmin-forerunner-35-a-competent-budget-running-watch" rel="alternate"></link><published>2020-11-13T22:35:00+00:00</published><updated>2020-11-13T22:35:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-11-13:/posts/garmin-forerunner-35-a-competent-budget-running-watch</id><summary type="html">&lt;p&gt;A review of the Garmin Forerunner 35 after using for several months,
comparing with the Forerunner 15.&lt;/p&gt;</summary><content type="html">&lt;h2 id="replacing-a-garmin-forerunner-15-watch"&gt;Replacing a Garmin Forerunner 15 watch&lt;/h2&gt;
&lt;p&gt;Unsurprisingly, I've been doing a lot more exercise outdoors this year
than last, including a lot more running. So I've had a lot more time to
try out the Garmin Forerunner 35 GPS watch I've had since the end of
2019.&lt;/p&gt;
&lt;p&gt;I have owned a &lt;a href="https://www.stevenmaude.co.uk/posts/a-year-of-garmin-a-forerunner-15-review"&gt;Garmin Forerunner
15&lt;/a&gt; watch for a few years and,
especially considering what it cost, it served me well. The main issue
with it was that the battery life seemed to have deteriorated over its
lifetime. Changing the battery isn't too complex a procedure — there are
YouTube videos that show you how — but since the battery life was never
that great from new, it seemed worthwhile looking into getting a
different watch. And late last year, I saw an offer on a Garmin
Forerunner 35. The Forerunner 35, like the Forerunner 15, is again an
entry-level GPS watch, but it's a considerable upgrade from the
Forerunner 15.&lt;/p&gt;
&lt;h2 id="comparing-the-forerunner-35-to-the-forerunner-15"&gt;Comparing the Forerunner 35 to the Forerunner 15&lt;/h2&gt;
&lt;p&gt;Though the Forerunner 35 is comparably priced to what the Forerunner 15
cost when I bought it, there are a number of improvements and extra
features of the Forerunner 35 over the Forerunner 15.&lt;/p&gt;
&lt;p&gt;The Forerunner 35 is a less chunky, and more subdued design than the
Forerunner 15 — looking around, there's a Forerunner 25 that looks like
an odd hybrid of the 15 and 35 designs — and has walking and outdoor
cycling modes. The screen on the 35 is also higher resolution. There's a
built-in wrist heart rate monitor. I'm not entirely sure the heart rate
measurement is always that accurate, but it removes the need for an
external sensor, if a rough idea of heart rate is what you want. The
backlight is also an improvement over the Forerunner 15. There's also
the option to pair a phone via Bluetooth, though that's not something
I've bothered to use.&lt;/p&gt;
&lt;p&gt;On using the 35, the battery life does seem much improved over the 15. I
never wore the Forerunner 15 daily, only for runs. By contrast, I'm
wearing the Forerunner 35 I've used daily: it does need a charge every
week or so, but that doesn't usually take long. Even if low on power,
you can give it a quick boost before you head out and it will probably
tide you over for your run. With the Forerunner 15, there were numerous
times where I found it would run out of battery on longer runs. &lt;/p&gt;
&lt;h2 id="using-the-forerunner-35-on-linux"&gt;Using the Forerunner 35 on Linux&lt;/h2&gt;
&lt;p&gt;Much like the old Forerunner 15, this watch still works fairly well even
if you're not using Garmin's own software. That might be because you're
not keen on sharing your data or because you're on Linux and can't
easily run the software; the Windows version may run with WINE, however.&lt;/p&gt;
&lt;p&gt;Even if you don't use Garmin Connect, the watch gives you access to
everything you need as it appears as a USB storage device. You can
retrieve and apply GPS updates by &lt;a href="https://github.com/StevenMaude/armstrong"&gt;other
means&lt;/a&gt;; without a regular
update, GPS locking can be slow and take several minutes, as it did with
the Forerunner 15. It's also straightforward to copy the Garmin &lt;code&gt;.FIT&lt;/code&gt;
files from the watch and convert them to &lt;code&gt;.gpx&lt;/code&gt; with GPSBabel or
similar.&lt;/p&gt;
&lt;h2 id="maintenance"&gt;Maintenance&lt;/h2&gt;
&lt;p&gt;The two things that usually fail on digital watches are the strap and
the battery. The Forerunner 35 has a reasonably durable strap. And the
strap's replaceable: you can at least buy third-party replacements. The
battery is another matter: Garmin don't want you to replace the battery
in any of their watches at all — and I can't find any posts or videos on
doing so for the 35. However, if the Forerunner 35 is built like the
Forerunner 15, a battery replacement could be possible if you're willing
to take the watch apart.&lt;/p&gt;
&lt;p&gt;The battery replacement issue is one reason why I'm not inclined to
spend much more on specialist watches. You can easily spend two to four
times as much as the Forerunner 35 costs on more expensive Garmin
watches. I'm not sure how much value you get for spending more. For me,
the only significant features missing from this watch are directions to
follow a GPX route, and a swim mode. (That said, the Forerunner 35, like
the Forerunner 15, is waterproof). Neither of those omissions are enough
for me to spend considerably more. My primary use is tracking pacing and
distance while running, and the Forerunner 35 does well here.&lt;/p&gt;
&lt;h2 id="overall"&gt;Overall?&lt;/h2&gt;
&lt;p&gt;Whether you're looking at replacing an older, budget running watch, or
you've not ever had such a watch and want to try one out without
spending too much, the Forerunner 35 is a quietly competent choice. It's
been a substantial upgrade from the Forerunner 15, which I also liked a
lot.&lt;/p&gt;</content><category term="2020"></category><category term="Forerunner"></category><category term="Garmin"></category><category term="review"></category><category term="running"></category></entry><entry><title>Continuous integration with GitHub Actions</title><link href="https://www.stevenmaude.co.uk/posts/continuous-integration-with-github-actions" rel="alternate"></link><published>2020-08-02T14:38:00+01:00</published><updated>2020-08-02T14:38:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-08-02:/posts/continuous-integration-with-github-actions</id><summary type="html">&lt;p&gt;A look at GitHub Actions, and the good and bad of coupling automation
to a remote repository provider&lt;/p&gt;</summary><content type="html">&lt;h2 id="a-quick-review-of-github-actions"&gt;A quick review of GitHub Actions&lt;/h2&gt;
&lt;p&gt;I've been using &lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt; a
lot recently. It's a continuous integration and continuous
delivery/deployment (CI/CD) tool that's, unsurprisingly, part of GitHub.&lt;/p&gt;
&lt;p&gt;As the vagueness of the "D" in "CD" might indicate, different people
have different interpretations of how you actually define these terms.
For this discussion, let's just take it that these tools encourage
developers to automate many software development processes that don't
involve writing code. There's an aim of frequently getting changes into
the main development branch, with minimal manual labour and aiming for a
good level of quality control, trying to avoid broken code getting
deployed.&lt;/p&gt;
&lt;p&gt;So, I've been using GitHub Actions a lot, both for personal projects and
at work. And I've enjoyed using it. It's reasonably easy to use, and
there's a quick feedback loop from you pushing a workflow to seeing it
run. This in turn makes it fun to work with; it puts you in a frame of
mind where you ponder what development tasks you could automate.&lt;/p&gt;
&lt;p&gt;Where GitHub Actions lacks is often for want of a little polish and
refinement are needed. I'll mention three examples. First, the
&lt;a href="https://docs.github.com/en/actions"&gt;documentation&lt;/a&gt; occasionally lacks
simple concrete examples for some of the features. The early parts of
the documentation are actually example-focused. But then many of the
features are simply all piled into a reference section at the end.
Additionally, I don't know what exactly it is about the page design or
the structuring of the documentation, but I did find it difficult to
find precisely what I was looking for. That was the case even when I'd
actually read some information before and knew it was hidden there,
somewhere.&lt;/p&gt;
&lt;p&gt;Often I've found more useful information elsewhere. Other helpful places
I've found have included: GitHub's Architecture Decision Records in the
&lt;a href="https://github.com/actions/runner"&gt;GitHub Actions Runner&lt;/a&gt; repository,
Stack Overflow, other people's GitHub Actions workflows, and
documentation for third-party actions.&lt;/p&gt;
&lt;p&gt;Second, at time of writing, there is no simple way to remove repetition
within workflows and between workflows. For instance, you might want to
run tests before you merge pull requests, and might want to do the same
when you build and publish a new release. At the moment, this means
duplicating part of one workflow in another, or creating a custom
action. GitHub are planning to add &lt;a href="https://github.com/actions/runner/issues/438"&gt;"composite"
actions&lt;/a&gt; soon which should
help simplify this type of common reuse.&lt;/p&gt;
&lt;p&gt;Finally, workflow steps do not have a non-failing "neutral" outcome.
(Apparently, this did use to be a feature during the GitHub Actions
beta.) You might design a workflow with a step that could determine
whether subsequent steps should run or not. Without a neutral outcome,
there is no easy way to indicate "no error, but we have stopped the
workflow early". The only way to directly exit the workflow at that
point is to fail. But this isn't semantically what is intended: this
type of exit may be an expected valid state and not a workflow failure.&lt;/p&gt;
&lt;p&gt;Alternatively, you can continue without failing by specifying
&lt;code&gt;continue-on-error: true&lt;/code&gt; for a step. But now all subsequent steps will
run, which we don't want: maybe those steps will waste a sizable chunk
of our Actions minutes quota and/or maybe those steps will fail too. To
skip the rest of the steps when they're not needed, conditional checks
on "success" of steps can be threaded through the subsequent steps of
your workflow. But this complicates the workflow, a "neutral" outcome
would simplify this entirely by allowing us to exit, without failure, at
the point where we really do want the workflow to stop.&lt;/p&gt;
&lt;h3 id="competitive-pricing-and-features"&gt;Competitive pricing and features&lt;/h3&gt;
&lt;p&gt;GitHub Actions does well on bundling free usage minutes, even for
private projects. For something like, say &lt;a href="https://travis-ci.com/"&gt;Travis
CI&lt;/a&gt;, you have to pay a hefty fixed price per
month to get that option. Though it should be noted that Travis CI
pricing includes unlimited minutes, whereas you have to pay if you
exceed your GitHub Actions monthly quota.&lt;/p&gt;
&lt;p&gt;However, at current pricing, you could get almost 10,000 monthly minutes
on GitHub Actions using a Linux runner — $0.008 per minute, with 2000
included in a GitHub Free plan — for the cost of the cheapest Travis
plan ($63). So, unless you're using a lot of minutes, when comparing on
price alone, there's no contest.&lt;/p&gt;
&lt;p&gt;The other big advantage of GitHub Actions is that it permits far more
concurrent jobs than Travis CI: at the time of writing, a GitHub Free
account has twenty concurrent (non-macOS) jobs. Travis CI offers only
ten concurrent jobs even at the highest "Premium" plan, and just one on
their lowest priced plan. This is important as, particularly if you're
collaborating in a team, your request for a CI job may be blocked by
other outstanding jobs, possibly delaying the completion of your
requested job.&lt;/p&gt;
&lt;p&gt;Both Travis CI and GitHub Actions offer free usage in some cases. GitHub
Actions pricing refers to "free for public repositories". Travis CI's
own site mentions "free for open source". I suspect by that this Travis
CI really just mean "your code must be public". And not that anyone
actually checks that your code is published under some kind of valid
open source license.&lt;/p&gt;
&lt;h3 id="coupling-automation-to-the-remote-repository-host"&gt;Coupling automation to the remote repository host&lt;/h3&gt;
&lt;p&gt;Price and concurrency aside, what might be consequences of using the
same service — here GitHub — for hosting the repository's code and that
repository's automation?&lt;/p&gt;
&lt;p&gt;From a quick search around, there doesn't appear any standard format for
specifying these types of automation workflows. That's not too
surprising. One reason might be that every provider implements their own
features and therefore wants to be able to flexibly specify the
configuration to incorporate those features. There aren't any
transpilers for different formats: I can't take an existing
&lt;code&gt;.travis.yml&lt;/code&gt; and automatically convert it to a GitHub Actions
&lt;code&gt;workflow.yml&lt;/code&gt;. (However, there is &lt;a href="https://github.com/ropensci/tic"&gt;an R
project&lt;/a&gt; that aims to specify workflow
configurations for R packages agnostically, outputting configuration for
different providers.)&lt;/p&gt;
&lt;p&gt;The consequence is that using GitHub Actions means that you now have a
coupling of your code's organisation to GitHub, and adds work should you
migrate to another Git repository host. To be fair, this is likely true
already, even without using GitHub Actions. If you move to a different
host, say GitLab, at the least, you'll also probably want to move the
existing issues from GitHub to the new host. In mitigation of this,
there are often tools or automatic imports for migrating these other
non-code features, like issues.&lt;/p&gt;
&lt;p&gt;But, as we've seen, any GitHub Actions workflows will probably need a
manual rewrite to use a similar process elsewhere. This, then, is
probably where using an external automation tool like Travis CI offers a
benefit. If you migrate to a new remote repository host, to get your
automation running, you just need to connect your new host to your
existing automation system. No rewrite required.&lt;/p&gt;
&lt;p&gt;On the other hand, it's reasonable to think that external tools are more
awkward to configure and monitor as opposed to comparable tooling that
is a key part of your existing repository host. With an external
service, there will also be some requirement for developers to create an
account there, and then allow the external application to access their
GitHub account. With GitHub Actions, it's just another tab on your
GitHub repository. (Travis CI certainly didn't help themselves here by
previously having &lt;a href="https://blog.travis-ci.com/2018-05-02-open-source-projects-on-travis-ci-com-with-github-apps"&gt;&lt;em&gt;two&lt;/em&gt; separate
domains&lt;/a&gt;,
to independently handle free and paid services.)&lt;/p&gt;
&lt;p&gt;And an external tool is an additional point of failure. It's possible
GitHub is up and running, while Travis CI is unavailable, or vice versa.
Either service failing can block active development.&lt;/p&gt;
&lt;h2 id="so-whats-best"&gt;So what's best?&lt;/h2&gt;
&lt;p&gt;Like many choices, there's no definitive answer. If the CI services have
roughly equivalent features, then it comes down to what other aspects
you prioritise. Is tight integration of your software development
processes a positive — making for potentially easier configuration — or
a hindrance to migration? Is pricing the most critical aspect?&lt;/p&gt;
&lt;p&gt;For funded development teams, the pricing issue is perhaps less
important since it will likely still be a small part of an
organisation's budget. Where GitHub Actions is particularly well placed
is in the giving of individual developers an allocation of free
automation job time for their private projects. Together with that
allocation being easily used as part of the service developers are
already likely using, it is challenging for external automation services
to compete.&lt;/p&gt;
&lt;p&gt;And that's where my personal view is. I'm still sticking with GitHub
Actions, notwithstanding the slight risk of getting too tied into
GitHub. GitHub is incredibly popular right now, it's where open source
projects are developed, and that network effect is an important one.&lt;/p&gt;</content><category term="2020"></category><category term="automation"></category><category term="continuous delivery"></category><category term="continuous integration"></category><category term="GitHub"></category><category term="GitHub Actions"></category></entry><entry><title>Oh, fork it</title><link href="https://www.stevenmaude.co.uk/posts/oh-fork-it" rel="alternate"></link><published>2020-06-19T12:35:00+01:00</published><updated>2020-06-19T20:39:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-06-19:/posts/oh-fork-it</id><summary type="html">&lt;p&gt;Why you should fork obscure, interesting repositories&lt;/p&gt;</summary><content type="html">&lt;h2 id="lost"&gt;Lost…&lt;/h2&gt;
&lt;p&gt;At work recently, I was trying to find a GitHub repository that I knew I
had previously seen a while back.&lt;/p&gt;
&lt;p&gt;But I couldn't find it.&lt;/p&gt;
&lt;p&gt;Normally, I would "star" possibly useful repositories: it wasn't in that
starred list. After spending a fair length of time trying to rediscover
the repository by many searches on GitHub and on search engines, I
concluded that maybe the author had simply deleted the repository or
their account.&lt;/p&gt;
&lt;p&gt;And so I gave up.&lt;/p&gt;
&lt;h2 id="and-found"&gt;…and found&lt;/h2&gt;
&lt;p&gt;Sometime later, while reading a related GitHub issue, I spotted I had
written a comment linking to the repository. It was still there, just
not easily discoverable.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id="lessons"&gt;Lessons&lt;/h2&gt;
&lt;p&gt;Things shared on the internet have no guarantee of longevity. You are
subject to the whims either of service providers either disappearing
entirely, or &lt;a href="https://www.stevenmaude.co.uk/posts/rinse-fms-soundcloud-account-takedown"&gt;removing a user and all their
content&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Users themselves may also delete things they've previously shared.
Especially in a post-GDPR world, where people are likely far more aware
of their ability and rights to do just that.&lt;/p&gt;
&lt;p&gt;Even if I had starred the elusive repository, that would not have helped
if the user deleted the repository or the user's account disappeared.
For popular repositories, there is no likely threat of them vanishing
entirely overnight, because there are probably several existing forks.
And users may well restore a copy of such a deleted repository from
local clones.&lt;/p&gt;
&lt;p&gt;But if a repository is obscure, then that published version may be the
only source readily available.&lt;/p&gt;
&lt;p&gt;So, if there is some GitHub — or other online Git remote — repository
that looks interesting or useful, but is relatively obscure, then
forking it is prudent, and a one click operation without requiring you
to store a copy locally.&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;It may not be that often when people or organisations decide to delete
all their code, but it &lt;a href="https://www.theregister.com/2016/03/23/npm_left_pad_chaos/"&gt;does
happen&lt;/a&gt;.
Even if you may not necessarily have the final version before deletion,
something may be better than a distant memory.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;After all that effort, I actually decided to use a different
  approach anyway.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Of course, cloning locally is another valid approach, but requires
  you to store content that may just be clutter on your local storage.
  I'd wager if GitHub was to end their service, then there would be
  more than a day's notice.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="2020"></category><category term="GitHub"></category></entry><entry><title>"The Missing Semester of Your CS Education": a course review</title><link href="https://www.stevenmaude.co.uk/posts/the-missing-semester-of-your-cs-education-a-course-review" rel="alternate"></link><published>2020-06-07T20:55:00+01:00</published><updated>2020-06-07T20:55:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-06-07:/posts/the-missing-semester-of-your-cs-education-a-course-review</id><summary type="html">&lt;p&gt;A quick review and recommendation of a useful computer science
course&lt;/p&gt;</summary><content type="html">&lt;h2 id="missing-semester-singular"&gt;Missing semester: singular?&lt;/h2&gt;
&lt;p&gt;Though I have a science background, I don't have a formal computer
science education at all. In that sense then, a course entitled &lt;a href="https://missing.csail.mit.edu/"&gt;"The
Missing Semester of Your CS Education"&lt;/a&gt;
is an underestimation.&lt;/p&gt;
&lt;p&gt;In my case, they're &lt;em&gt;all&lt;/em&gt; missing semesters. (I'd love to learn more and
I do keep trying to learn what I can at work, and pick up new things in
my spare time.)&lt;/p&gt;
&lt;p&gt;But, looking over the course syllabus, I realised that, actually, I'd
already covered a fair amount of what it covers. Much of this had been
via learning at work, often from colleagues telling me about particular
tools. That actually sold the course to me: I was already aware that
some of the chosen topics were particularly useful. So, I figured, what
I didn't know might be worth knowing too.&lt;/p&gt;
&lt;p&gt;The central theme of the course is around the tooling that developers,
especially on Linux, might use while developing software, or working at
the command-line. The course covers: working with the command-line shell
and shell scripting, text editors, version control with Git, using
debugging and profiling tools, and a little introduction to
cryptographic tools.&lt;/p&gt;
&lt;h2 id="course-format"&gt;Course format&lt;/h2&gt;
&lt;p&gt;With eleven lectures of about an hour each, the course is well presented
and not too lengthy. The lecture notes available on the &lt;a href="https://missing.csail.mit.edu/"&gt;course
site&lt;/a&gt; are a useful reference and usually
fairly comprehensive. The notes don't always cover everything in the
lectures, particularly where there are worked examples. But, a skim
through the notes should give you a good impression of what's discussed,
if you prefer reading to videos.&lt;/p&gt;
&lt;h2 id="what-i-liked-about-the-course"&gt;What I liked about the course&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Many of the lectures are self-contained. There are a few that are
  easier to follow with some basic command-line shell knowledge, but
  that too is covered in the first couple of lectures.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consequently, this means the course works well as a survey of topics.
  There is a little detail imparted to give you some deeper background,
  and sometimes other recommended readings. However, the level of
  presentation and the rate at which the material is worked through is
  very approachable without being overwhelming.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The exercises for each lecture apply the ideas covered in a very
  direct and practical way. Despite the problems being artificial, it is
  possible to envisage that you might solve real problems with similar
  approaches.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="do-i-recommend-it"&gt;Do I recommend it?&lt;/h2&gt;
&lt;p&gt;Yes, definitely. It will probably take about ten to twenty hours
covering this material, depending on whether you watch all the videos or
just read the notes. I should emphasise again that, even if you don't
want to go through the entire course, you can easily pick out the few
lectures that might interest you.&lt;/p&gt;
&lt;p&gt;I did learn a few things that I didn't know. And the exercises allowed
me to try working with some tools I hadn't used before (e.g. Linux
kernel cgroups).&lt;/p&gt;
&lt;p&gt;If you're an experienced Linux developer and constantly keep up-to-date
with tools, there may not be much new here. However, for developers
starting out, those who are starting to use Linux, those who are still
in education or those, like me, who think they might have some knowledge
gaps, this course is a nice compliment to other learning resources.&lt;/p&gt;</content><category term="2020"></category><category term="computer science"></category><category term="course"></category><category term="Linux"></category></entry></feed>