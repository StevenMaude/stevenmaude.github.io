<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>stevenmaude.co.uk</title><link href="https://www.stevenmaude.co.uk/" rel="alternate"></link><link href="https://www.stevenmaude.co.uk/feeds/all.atom.xml" rel="self"></link><id>https://www.stevenmaude.co.uk/</id><updated>2020-11-13T22:35:00+00:00</updated><entry><title>Garmin Forerunner 35: a competent budget running watch</title><link href="https://www.stevenmaude.co.uk/posts/garmin-forerunner-35-a-competent-budget-running-watch" rel="alternate"></link><published>2020-11-13T22:35:00+00:00</published><updated>2020-11-13T22:35:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-11-13:/posts/garmin-forerunner-35-a-competent-budget-running-watch</id><summary type="html">&lt;p&gt;A review of the Garmin Forerunner 35 after using for several months,
comparing with the Forerunner 15.&lt;/p&gt;</summary><content type="html">&lt;h2 id="replacing-a-garmin-forerunner-15-watch"&gt;Replacing a Garmin Forerunner 15 watch&lt;/h2&gt;
&lt;p&gt;Unsurprisingly, I've been doing a lot more exercise outdoors this year
than last, including a lot more running. So I've had a lot more time to
try out the Garmin Forerunner 35 GPS watch I've had since the end of
2019.&lt;/p&gt;
&lt;p&gt;I have owned a &lt;a href="https://www.stevenmaude.co.uk/posts/a-year-of-garmin-a-forerunner-15-review"&gt;Garmin Forerunner
15&lt;/a&gt; watch for a few years and,
especially considering what it cost, it served me well. The main issue
with it was that the battery life seemed to have deteriorated over its
lifetime. Changing the battery isn't too complex a procedure — there are
YouTube videos that show you how — but since the battery life was never
that great from new, it seemed worthwhile looking into getting a
different watch. And late last year, I saw an offer on a Garmin
Forerunner 35. The Forerunner 35, like the Forerunner 15, is again an
entry-level GPS watch, but it's a considerable upgrade from the
Forerunner 15.&lt;/p&gt;
&lt;h2 id="comparing-the-forerunner-35-to-the-forerunner-15"&gt;Comparing the Forerunner 35 to the Forerunner 15&lt;/h2&gt;
&lt;p&gt;Though the Forerunner 35 is comparably priced to what the Forerunner 15
cost when I bought it, there are a number of improvements and extra
features of the Forerunner 35 over the Forerunner 15.&lt;/p&gt;
&lt;p&gt;The Forerunner 35 is a less chunky, and more subdued design than the
Forerunner 15 — looking around, there's a Forerunner 25 that looks like
an odd hybrid of the 15 and 35 designs — and has walking and outdoor
cycling modes. The screen on the 35 is also higher resolution. There's a
built-in wrist heart rate monitor. I'm not entirely sure the heart rate
measurement is always that accurate, but it removes the need for an
external sensor, if a rough idea of heart rate is what you want. The
backlight is also an improvement over the Forerunner 15. There's also
the option to pair a phone via Bluetooth, though that's not something
I've bothered to use.&lt;/p&gt;
&lt;p&gt;On using the 35, the battery life does seem much improved over the 15. I
never wore the Forerunner 15 daily, only for runs. By contrast, I'm
wearing the Forerunner 35 I've used daily: it does need a charge every
week or so, but that doesn't usually take long. Even if low on power,
you can give it a quick boost before you head out and it will probably
tide you over for your run. With the Forerunner 15, there were numerous
times where I found it would run out of battery on longer runs. &lt;/p&gt;
&lt;h2 id="using-the-forerunner-35-on-linux"&gt;Using the Forerunner 35 on Linux&lt;/h2&gt;
&lt;p&gt;Much like the old Forerunner 15, this watch still works fairly well even
if you're not using Garmin's own software. That might be because you're
not keen on sharing your data or because you're on Linux and can't
easily run the software; the Windows version may run with WINE, however.&lt;/p&gt;
&lt;p&gt;Even if you don't use Garmin Connect, the watch gives you access to
everything you need as it appears as a USB storage device. You can
retrieve and apply GPS updates by &lt;a href="https://github.com/StevenMaude/armstrong"&gt;other
means&lt;/a&gt;; without a regular
update, GPS locking can be slow and take several minutes, as it did with
the Forerunner 15. It's also straightforward to copy the Garmin &lt;code&gt;.FIT&lt;/code&gt;
files from the watch and convert them to &lt;code&gt;.gpx&lt;/code&gt; with GPSBabel or
similar.&lt;/p&gt;
&lt;h2 id="maintenance"&gt;Maintenance&lt;/h2&gt;
&lt;p&gt;The two things that usually fail on digital watches are the strap and
the battery. The Forerunner 35 has a reasonably durable strap. And the
strap's replaceable: you can at least buy third-party replacements. The
battery is another matter: Garmin don't want you to replace the battery
in any of their watches at all — and I can't find any posts or videos on
doing so for the 35. However, if the Forerunner 35 is built like the
Forerunner 15, a battery replacement could be possible if you're willing
to take the watch apart.&lt;/p&gt;
&lt;p&gt;The battery replacement issue is one reason why I'm not inclined to
spend much more on specialist watches. You can easily spend two to four
times as much as the Forerunner 35 costs on more expensive Garmin
watches. I'm not sure how much value you get for spending more. For me,
the only significant features missing from this watch are directions to
follow a GPX route, and a swim mode. (That said, the Forerunner 35, like
the Forerunner 15, is waterproof). Neither of those omissions are enough
for me to spend considerably more. My primary use is tracking pacing and
distance while running, and the Forerunner 35 does well here.&lt;/p&gt;
&lt;h2 id="overall"&gt;Overall?&lt;/h2&gt;
&lt;p&gt;Whether you're looking at replacing an older, budget running watch, or
you've not ever had such a watch and want to try one out without
spending too much, the Forerunner 35 is a quietly competent choice. It's
been a substantial upgrade from the Forerunner 15, which I also liked a
lot.&lt;/p&gt;</content><category term="2020"></category><category term="Forerunner"></category><category term="Garmin"></category><category term="review"></category><category term="running"></category></entry><entry><title>Continuous integration with GitHub Actions</title><link href="https://www.stevenmaude.co.uk/posts/continuous-integration-with-github-actions" rel="alternate"></link><published>2020-08-02T14:38:00+01:00</published><updated>2020-08-02T14:38:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-08-02:/posts/continuous-integration-with-github-actions</id><summary type="html">&lt;p&gt;A look at GitHub Actions, and the good and bad of coupling automation
to a remote repository provider&lt;/p&gt;</summary><content type="html">&lt;h2 id="a-quick-review-of-github-actions"&gt;A quick review of GitHub Actions&lt;/h2&gt;
&lt;p&gt;I've been using &lt;a href="https://github.com/features/actions"&gt;GitHub Actions&lt;/a&gt; a
lot recently. It's a continuous integration and continuous
delivery/deployment (CI/CD) tool that's, unsurprisingly, part of GitHub.&lt;/p&gt;
&lt;p&gt;As the vagueness of the "D" in "CD" might indicate, different people
have different interpretations of how you actually define these terms.
For this discussion, let's just take it that these tools encourage
developers to automate many software development processes that don't
involve writing code. There's an aim of frequently getting changes into
the main development branch, with minimal manual labour and aiming for a
good level of quality control, trying to avoid broken code getting
deployed.&lt;/p&gt;
&lt;p&gt;So, I've been using GitHub Actions a lot, both for personal projects and
at work. And I've enjoyed using it. It's reasonably easy to use, and
there's a quick feedback loop from you pushing a workflow to seeing it
run. This in turn makes it fun to work with; it puts you in a frame of
mind where you ponder what development tasks you could automate.&lt;/p&gt;
&lt;p&gt;Where GitHub Actions lacks is often for want of a little polish and
refinement are needed. I'll mention three examples. First, the
&lt;a href="https://docs.github.com/en/actions"&gt;documentation&lt;/a&gt; occasionally lacks
simple concrete examples for some of the features. The early parts of
the documentation are actually example-focused. But then many of the
features are simply all piled into a reference section at the end.
Additionally, I don't know what exactly it is about the page design or
the structuring of the documentation, but I did find it difficult to
find precisely what I was looking for. That was the case even when I'd
actually read some information before and knew it was hidden there,
somewhere.&lt;/p&gt;
&lt;p&gt;Often I've found more useful information elsewhere. Other helpful places
I've found have included: GitHub's Architecture Decision Records in the
&lt;a href="https://github.com/actions/runner"&gt;GitHub Actions Runner&lt;/a&gt; repository,
Stack Overflow, other people's GitHub Actions workflows, and
documentation for third-party actions.&lt;/p&gt;
&lt;p&gt;Second, at time of writing, there is no simple way to remove repetition
within workflows and between workflows. For instance, you might want to
run tests before you merge pull requests, and might want to do the same
when you build and publish a new release. At the moment, this means
duplicating part of one workflow in another, or creating a custom
action. GitHub are planning to add &lt;a href="https://github.com/actions/runner/issues/438"&gt;"composite"
actions&lt;/a&gt; soon which should
help simplify this type of common reuse.&lt;/p&gt;
&lt;p&gt;Finally, workflow steps do not have a non-failing "neutral" outcome.
(Apparently, this did use to be a feature during the GitHub Actions
beta.) You might design a workflow with a step that could determine
whether subsequent steps should run or not. Without a neutral outcome,
there is no easy way to indicate "no error, but we have stopped the
workflow early". The only way to directly exit the workflow at that
point is to fail. But this isn't semantically what is intended: this
type of exit may be an expected valid state and not a workflow failure.&lt;/p&gt;
&lt;p&gt;Alternatively, you can continue without failing by specifying
&lt;code&gt;continue-on-error: true&lt;/code&gt; for a step. But now all subsequent steps will
run, which we don't want: maybe those steps will waste a sizable chunk
of our Actions minutes quota and/or maybe those steps will fail too. To
skip the rest of the steps when they're not needed, conditional checks
on "success" of steps can be threaded through the subsequent steps of
your workflow. But this complicates the workflow, a "neutral" outcome
would simplify this entirely by allowing us to exit, without failure, at
the point where we really do want the workflow to stop.&lt;/p&gt;
&lt;h3 id="competitive-pricing-and-features"&gt;Competitive pricing and features&lt;/h3&gt;
&lt;p&gt;GitHub Actions does well on bundling free usage minutes, even for
private projects. For something like, say &lt;a href="https://travis-ci.com/"&gt;Travis
CI&lt;/a&gt;, you have to pay a hefty fixed price per
month to get that option. Though it should be noted that Travis CI
pricing includes unlimited minutes, whereas you have to pay if you
exceed your GitHub Actions monthly quota.&lt;/p&gt;
&lt;p&gt;However, at current pricing, you could get almost 10,000 monthly minutes
on GitHub Actions using a Linux runner — $0.008 per minute, with 2000
included in a GitHub Free plan — for the cost of the cheapest Travis
plan ($63). So, unless you're using a lot of minutes, when comparing on
price alone, there's no contest.&lt;/p&gt;
&lt;p&gt;The other big advantage of GitHub Actions is that it permits far more
concurrent jobs than Travis CI: at the time of writing, a GitHub Free
account has twenty concurrent (non-macOS) jobs. Travis CI offers only
ten concurrent jobs even at the highest "Premium" plan, and just one on
their lowest priced plan. This is important as, particularly if you're
collaborating in a team, your request for a CI job may be blocked by
other outstanding jobs, possibly delaying the completion of your
requested job.&lt;/p&gt;
&lt;p&gt;Both Travis CI and GitHub Actions offer free usage in some cases. GitHub
Actions pricing refers to "free for public repositories". Travis CI's
own site mentions "free for open source". I suspect by that this Travis
CI really just mean "your code must be public". And not that anyone
actually checks that your code is published under some kind of valid
open source license.&lt;/p&gt;
&lt;h3 id="coupling-automation-to-the-remote-repository-host"&gt;Coupling automation to the remote repository host&lt;/h3&gt;
&lt;p&gt;Price and concurrency aside, what might be consequences of using the
same service — here GitHub — for hosting the repository's code and that
repository's automation?&lt;/p&gt;
&lt;p&gt;From a quick search around, there doesn't appear any standard format for
specifying these types of automation workflows. That's not too
surprising. One reason might be that every provider implements their own
features and therefore wants to be able to flexibly specify the
configuration to incorporate those features. There aren't any
transpilers for different formats: I can't take an existing
&lt;code&gt;.travis.yml&lt;/code&gt; and automatically convert it to a GitHub Actions
&lt;code&gt;workflow.yml&lt;/code&gt;. (However, there is &lt;a href="https://github.com/ropensci/tic"&gt;an R
project&lt;/a&gt; that aims to specify workflow
configurations for R packages agnostically, outputting configuration for
different providers.)&lt;/p&gt;
&lt;p&gt;The consequence is that using GitHub Actions means that you now have a
coupling of your code's organisation to GitHub, and adds work should you
migrate to another Git repository host. To be fair, this is likely true
already, even without using GitHub Actions. If you move to a different
host, say GitLab, at the least, you'll also probably want to move the
existing issues from GitHub to the new host. In mitigation of this,
there are often tools or automatic imports for migrating these other
non-code features, like issues.&lt;/p&gt;
&lt;p&gt;But, as we've seen, any GitHub Actions workflows will probably need a
manual rewrite to use a similar process elsewhere. This, then, is
probably where using an external automation tool like Travis CI offers a
benefit. If you migrate to a new remote repository host, to get your
automation running, you just need to connect your new host to your
existing automation system. No rewrite required.&lt;/p&gt;
&lt;p&gt;On the other hand, it's reasonable to think that external tools are more
awkward to configure and monitor as opposed to comparable tooling that
is a key part of your existing repository host. With an external
service, there will also be some requirement for developers to create an
account there, and then allow the external application to access their
GitHub account. With GitHub Actions, it's just another tab on your
GitHub repository. (Travis CI certainly didn't help themselves here by
previously having &lt;a href="https://blog.travis-ci.com/2018-05-02-open-source-projects-on-travis-ci-com-with-github-apps"&gt;&lt;em&gt;two&lt;/em&gt; separate
domains&lt;/a&gt;,
to independently handle free and paid services.)&lt;/p&gt;
&lt;p&gt;And an external tool is an additional point of failure. It's possible
GitHub is up and running, while Travis CI is unavailable, or vice versa.
Either service failing can block active development.&lt;/p&gt;
&lt;h2 id="so-whats-best"&gt;So what's best?&lt;/h2&gt;
&lt;p&gt;Like many choices, there's no definitive answer. If the CI services have
roughly equivalent features, then it comes down to what other aspects
you prioritise. Is tight integration of your software development
processes a positive — making for potentially easier configuration — or
a hindrance to migration? Is pricing the most critical aspect?&lt;/p&gt;
&lt;p&gt;For funded development teams, the pricing issue is perhaps less
important since it will likely still be a small part of an
organisation's budget. Where GitHub Actions is particularly well placed
is in the giving of individual developers an allocation of free
automation job time for their private projects. Together with that
allocation being easily used as part of the service developers are
already likely using, it is challenging for external automation services
to compete.&lt;/p&gt;
&lt;p&gt;And that's where my personal view is. I'm still sticking with GitHub
Actions, notwithstanding the slight risk of getting too tied into
GitHub. GitHub is incredibly popular right now, it's where open source
projects are developed, and that network effect is an important one.&lt;/p&gt;</content><category term="2020"></category><category term="automation"></category><category term="continuous delivery"></category><category term="continuous integration"></category><category term="GitHub"></category><category term="GitHub Actions"></category></entry><entry><title>Oh, fork it</title><link href="https://www.stevenmaude.co.uk/posts/oh-fork-it" rel="alternate"></link><published>2020-06-19T12:35:00+01:00</published><updated>2020-06-19T20:39:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-06-19:/posts/oh-fork-it</id><summary type="html">&lt;p&gt;Why you should fork obscure, interesting repositories&lt;/p&gt;</summary><content type="html">&lt;h2 id="lost"&gt;Lost…&lt;/h2&gt;
&lt;p&gt;At work recently, I was trying to find a GitHub repository that I knew I
had previously seen a while back.&lt;/p&gt;
&lt;p&gt;But I couldn't find it.&lt;/p&gt;
&lt;p&gt;Normally, I would "star" possibly useful repositories: it wasn't in that
starred list. After spending a fair length of time trying to rediscover
the repository by many searches on GitHub and on search engines, I
concluded that maybe the author had simply deleted the repository or
their account.&lt;/p&gt;
&lt;p&gt;And so I gave up.&lt;/p&gt;
&lt;h2 id="and-found"&gt;…and found&lt;/h2&gt;
&lt;p&gt;Sometime later, while reading a related GitHub issue, I spotted I had
written a comment linking to the repository. It was still there, just
not easily discoverable.&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h2 id="lessons"&gt;Lessons&lt;/h2&gt;
&lt;p&gt;Things shared on the internet have no guarantee of longevity. You are
subject to the whims either of service providers either disappearing
entirely, or &lt;a href="https://www.stevenmaude.co.uk/posts/rinse-fms-soundcloud-account-takedown"&gt;removing a user and all their
content&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Users themselves may also delete things they've previously shared.
Especially in a post-GDPR world, where people are likely far more aware
of their ability and rights to do just that.&lt;/p&gt;
&lt;p&gt;Even if I had starred the elusive repository, that would not have helped
if the user deleted the repository or the user's account disappeared.
For popular repositories, there is no likely threat of them vanishing
entirely overnight, because there are probably several existing forks.
And users may well restore a copy of such a deleted repository from
local clones.&lt;/p&gt;
&lt;p&gt;But if a repository is obscure, then that published version may be the
only source readily available.&lt;/p&gt;
&lt;p&gt;So, if there is some GitHub — or other online Git remote — repository
that looks interesting or useful, but is relatively obscure, then
forking it is prudent, and a one click operation without requiring you
to store a copy locally.&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;It may not be that often when people or organisations decide to delete
all their code, but it &lt;a href="https://www.theregister.com/2016/03/23/npm_left_pad_chaos/"&gt;does
happen&lt;/a&gt;.
Even if you may not necessarily have the final version before deletion,
something may be better than a distant memory.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;After all that effort, I actually decided to use a different
  approach anyway.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Of course, cloning locally is another valid approach, but requires
  you to store content that may just be clutter on your local storage.
  I'd wager if GitHub was to end their service, then there would be
  more than a day's notice.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="2020"></category><category term="GitHub"></category></entry><entry><title>"The Missing Semester of Your CS Education": a course review</title><link href="https://www.stevenmaude.co.uk/posts/the-missing-semester-of-your-cs-education-a-course-review" rel="alternate"></link><published>2020-06-07T20:55:00+01:00</published><updated>2020-06-07T20:55:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-06-07:/posts/the-missing-semester-of-your-cs-education-a-course-review</id><summary type="html">&lt;p&gt;A quick review and recommendation of a useful computer science
course&lt;/p&gt;</summary><content type="html">&lt;h2 id="missing-semester-singular"&gt;Missing semester: singular?&lt;/h2&gt;
&lt;p&gt;Though I have a science background, I don't have a formal computer
science education at all. In that sense then, a course entitled &lt;a href="https://missing.csail.mit.edu/"&gt;"The
Missing Semester of Your CS Education"&lt;/a&gt;
is an underestimation.&lt;/p&gt;
&lt;p&gt;In my case, they're &lt;em&gt;all&lt;/em&gt; missing semesters. (I'd love to learn more and
I do keep trying to learn what I can at work, and pick up new things in
my spare time.)&lt;/p&gt;
&lt;p&gt;But, looking over the course syllabus, I realised that, actually, I'd
already covered a fair amount of what it covers. Much of this had been
via learning at work, often from colleagues telling me about particular
tools. That actually sold the course to me: I was already aware that
some of the chosen topics were particularly useful. So, I figured, what
I didn't know might be worth knowing too.&lt;/p&gt;
&lt;p&gt;The central theme of the course is around the tooling that developers,
especially on Linux, might use while developing software, or working at
the command-line. The course covers: working with the command-line shell
and shell scripting, text editors, version control with Git, using
debugging and profiling tools, and a little introduction to
cryptographic tools.&lt;/p&gt;
&lt;h2 id="course-format"&gt;Course format&lt;/h2&gt;
&lt;p&gt;With eleven lectures of about an hour each, the course is well presented
and not too lengthy. The lecture notes available on the &lt;a href="https://missing.csail.mit.edu/"&gt;course
site&lt;/a&gt; are a useful reference and usually
fairly comprehensive. The notes don't always cover everything in the
lectures, particularly where there are worked examples. But, a skim
through the notes should give you a good impression of what's discussed,
if you prefer reading to videos.&lt;/p&gt;
&lt;h2 id="what-i-liked-about-the-course"&gt;What I liked about the course&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Many of the lectures are self-contained. There are a few that are
  easier to follow with some basic command-line shell knowledge, but
  that too is covered in the first couple of lectures.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consequently, this means the course works well as a survey of topics.
  There is a little detail imparted to give you some deeper background,
  and sometimes other recommended readings. However, the level of
  presentation and the rate at which the material is worked through is
  very approachable without being overwhelming.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The exercises for each lecture apply the ideas covered in a very
  direct and practical way. Despite the problems being artificial, it is
  possible to envisage that you might solve real problems with similar
  approaches.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="do-i-recommend-it"&gt;Do I recommend it?&lt;/h2&gt;
&lt;p&gt;Yes, definitely. It will probably take about ten to twenty hours
covering this material, depending on whether you watch all the videos or
just read the notes. I should emphasise again that, even if you don't
want to go through the entire course, you can easily pick out the few
lectures that might interest you.&lt;/p&gt;
&lt;p&gt;I did learn a few things that I didn't know. And the exercises allowed
me to try working with some tools I hadn't used before (e.g. Linux
kernel cgroups).&lt;/p&gt;
&lt;p&gt;If you're an experienced Linux developer and constantly keep up-to-date
with tools, there may not be much new here. However, for developers
starting out, those who are starting to use Linux, those who are still
in education or those, like me, who think they might have some knowledge
gaps, this course is a nice compliment to other learning resources.&lt;/p&gt;</content><category term="2020"></category><category term="computer science"></category><category term="course"></category><category term="Linux"></category></entry><entry><title>Strange symptoms of hard drive enclosure failure</title><link href="https://www.stevenmaude.co.uk/posts/strange-symptoms-of-hard-drive-enclosure-failure" rel="alternate"></link><published>2020-05-09T21:35:00+01:00</published><updated>2020-05-09T21:35:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2020-05-09:/posts/strange-symptoms-of-hard-drive-enclosure-failure</id><summary type="html">&lt;p&gt;Describing some symptoms of a wonky hard drive enclosure on
Windows&lt;/p&gt;</summary><content type="html">&lt;h1 id="reliable-failure"&gt;Reliable failure&lt;/h1&gt;
&lt;p&gt;It's sometimes a reassuring comfort to know that computers and
computer-related things are entirely reliable. In that they are anything
but.&lt;/p&gt;
&lt;p&gt;As I spent about an hour looking into this problem, and now think it has
been fixed, perhaps it is helpful to document the symptoms such that
anyone searching might find this page and save them some time.
Especially if, like me, you don't have a spare enclosure to test with
and are thinking whether to buy a new one or not.&lt;/p&gt;
&lt;p&gt;(Yes, you probably should, is the answer, if you don't want to read
further.)&lt;/p&gt;
&lt;h1 id="some-background"&gt;Some background&lt;/h1&gt;
&lt;p&gt;My enclosure was a cheap and cheerful once with USB and eSATA ports.
eSATA is a bit dated these days, but is useful if you're working with
old PCs that don't have USB 3 ports.&lt;/p&gt;
&lt;h2 id="symptoms"&gt;Symptoms&lt;/h2&gt;
&lt;p&gt;There were different failure modes depending on how the drive was
connected:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When connected by USB, the drive was not recognised by Windows. That
  is, it did not show up in File Explorer when connected, as if it
  wasn't attached at all.&lt;/p&gt;
&lt;p&gt;In Disk Management, Windows was claiming the partition was a GPT
Protective Partition, whereas it was a real GPT partition, and
incorrectly had a size of 16,777,216 MB, i.e. the NTFS volume limit
with default cluster size, which was also incorrect.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When connected by eSATA, the drive was correctly recognised by
  Windows. Creating folders and, I think, copying moderate sized files
  apparently worked OK, though I didn't try reading the files back from
  the drive.&lt;/p&gt;
&lt;p&gt;However, running the Windows 7 Backup and Restore tool reliably caused
an error. Often, as soon as the backup process started to write to the
drive in the enclosure, the drive acted as if it had been
disconnected. That is, the backup failed, and the drive was no longer
visible in File Explorer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="possible-causes"&gt;Possible causes?&lt;/h2&gt;
&lt;p&gt;As far as I could tell, there were three possibilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Something was wrong with Windows&lt;/strong&gt;. Not impossible, but this felt
  like more of a hardware fault, especially with the strange difference
  in behaviour between USB and eSATA, and with observed failures when
  different parts of Windows were running. It wasn't just the Backup and
  Restore tool that was failing, but Disk Cleanup had also caused the
  drive to disconnect.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The hard drive itself was failing&lt;/strong&gt;. Not impossible either, but
  unlikely. Though a magnetic disk, it is enterprise-grade and hadn't
  had much usage. In fact, the drive's SMART readings indicated a few
  hundred hours of time spent powered on.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Something was wrong with the enclosure&lt;/strong&gt;. Probably most likely,
  given the enclosure was inexpensive.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A cabling issue was ruled out on the basis that both USB and eSATA
weren't working, and had very different behaviour via the two
connections.&lt;/p&gt;
&lt;h2 id="closing-off-the-matter"&gt;Closing off the matter&lt;/h2&gt;
&lt;p&gt;In the end, not having a spare enclosure to test with, I bought a new
enclosure to try out. Out of curiosity, I chose one with both USB and
eSATA to test both out, in case the problem recurred.&lt;/p&gt;
&lt;p&gt;This did resolve part of the problem: the drive showed up both via eSATA
and USB in the new enclosure. On the other hand, the Windows 7 Backup
tool on Windows 10 was still failing. My hunch was that the faulty
enclosure had somehow got the drive into an odd state. I never figured
out what exactly the problem was here. A disk check did show errors,
though I can't remember whether those were able to be repaired, or
whether the drive disconnected while trying to do so. Maybe there was
some corruption of the file system?&lt;/p&gt;
&lt;p&gt;In the end, formatting the drive, encrypting the drive again and then
backing up finally resolved the — by-now very tedious — problem. So, not
all that exciting, but if you are witnessing similar symptoms, you can
probably fix it with a new enclosure.&lt;/p&gt;</content><category term="2020"></category><category term="hard drive"></category><category term="hardware"></category><category term="Windows"></category></entry><entry><title>A ULPS take: Windows 10 black screens and slow boot</title><link href="https://www.stevenmaude.co.uk/posts/a-ulps-take-windows-10-black-screens-and-slow-boot" rel="alternate"></link><published>2019-12-21T12:29:00+00:00</published><updated>2019-12-21T12:29:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2019-12-21:/posts/a-ulps-take-windows-10-black-screens-and-slow-boot</id><summary type="html">&lt;p&gt;Disabling ULPS on laptops that Windows 10 doesn't play nicely with.&lt;/p&gt;</summary><content type="html">&lt;h1 id="a-slow-boot-problem"&gt;A slow boot problem&lt;/h1&gt;
&lt;p&gt;Recently I was helping a friend with an older laptop running Windows 10
that seemed unusually slow to start.&lt;/p&gt;
&lt;p&gt;Though Windows seemed nimble once it started, the boot and shutdown
times felt excessively slow: minutes instead of the seconds it should
take. And it wasn't like Windows itself was doing anything in this state
either: the machine was silent, instead of the fans powering up, and
there was just a black screen following the Windows logo.&lt;/p&gt;
&lt;p&gt;Fortunately, with some frantic searching, I stumbled on a potential
solution: the cause might be a particular AMD graphics problem,
especially with two graphics cards, integrated and discrete. And the
laptop I was looking at did indeed have AMD Radeon hardware.&lt;/p&gt;
&lt;h1 id="ulps"&gt;ULPS&lt;/h1&gt;
&lt;p&gt;ULPS is an initialism used by AMD to refer to Ultra Low Power State, and
is the feature that can lead to this slow boot problem. As the name
suggests, this is a power saving feature.&lt;/p&gt;
&lt;p&gt;With the particular Windows install I was looking at, I didn't try
updating drivers beyond those that were automatically found by Windows
itself. However, reading around, the drivers may never have been fixed
for legacy hardware anyway.&lt;/p&gt;
&lt;h1 id="the-fix"&gt;The fix&lt;/h1&gt;
&lt;p&gt;The fix I found was to disable this ULPS feature in the registry.&lt;/p&gt;
&lt;p&gt;First, open the Windows Registry Editor. (In the Windows 10 search box,
search for "regedit" and then run "Registry Editor"; I can't remember,
but these edits may require you to be running the Registry Editor as
administrator, so you may need to right-click on "Registry Editor" and
then choose to run it as administrator.)&lt;/p&gt;
&lt;p&gt;Search for &lt;code&gt;EnableULPS&lt;/code&gt;, set any &lt;code&gt;1&lt;/code&gt; values to &lt;code&gt;0&lt;/code&gt;. There are also
&lt;code&gt;EnableULPS_NA&lt;/code&gt; settings but I read conflicting reports of whether to
set these to &lt;code&gt;0&lt;/code&gt; or not; in the end, only changing the &lt;code&gt;EnableULPS&lt;/code&gt;
settings was sufficient to resolve this issue for the laptop I was
working with.&lt;/p&gt;
&lt;p&gt;Hopefully, that should cure the problem. It's certainly a possibility
that subsequently installed graphics drivers could reset these overrides
and cause the symptom to recur, but, if that's the case, the problem is
easy to spot.&lt;/p&gt;
&lt;p&gt;After disabling ULPS, both booting and shutdown were much, &lt;em&gt;much&lt;/em&gt;
faster. The downside is that disabling ULPS may make the battery deplete
more rapidly than if it were enabled. But the boot up and shutdown times
were so slow without this fix that I think it's a necessary trade-off
to make your PC usable day-to-day, should you require it.&lt;/p&gt;
&lt;p&gt;There are other causes of a slow booting Windows installation, but if
you are dealing with a slow starting PC with Radeon graphics,
particularly with two display adapters, it's worth trying to update the
drivers and, failing that, seeing if this fix works.&lt;/p&gt;</content><category term="2019"></category><category term="Windows"></category></entry><entry><title>Learning from other projects: pelican-themes</title><link href="https://www.stevenmaude.co.uk/posts/learning-from-other-projects-pelican-themes" rel="alternate"></link><published>2019-12-16T22:41:00+00:00</published><updated>2019-12-16T22:41:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2019-12-16:/posts/learning-from-other-projects-pelican-themes</id><summary type="html">&lt;p&gt;Taking a look at pelican-themes — a project I currently use — to learn from it.&lt;/p&gt;</summary><content type="html">&lt;h2 id="lack-of-updates"&gt;Lack of updates&lt;/h2&gt;
&lt;p&gt;At the time of writing, this blog uses Pelican to convert the Markdown
blog content into publishable HTML. And, if you noticed from the dates
on my posts, there has been just over a year between the last post and
this one. This was mainly because there was considerable work needed to
get this blog setup up-to-date: both in upgrading Pelican to the latest
version, and pulling in updates to the theme.&lt;/p&gt;
&lt;p&gt;Updating the theme was made slightly more complicated by the migration
of the theme from its &lt;a href="https://github.com/DandyDev/pelican-bootstrap3"&gt;original
repository&lt;/a&gt;. The new
home became the
&lt;a href="https://github.com/getpelican/pelican-themes"&gt;pelican-themes&lt;/a&gt;
repository, not as a submodule but as the definitive repository for that
theme.&lt;/p&gt;
&lt;p&gt;In the past, it was simpler for me to simply pull request changes from
the parent of my theme's fork directly into my fork. Instead, I had to
restructure my repository slightly and start to &lt;code&gt;git cherry-pick&lt;/code&gt;
individual commits that were applied to the theme within the repository.&lt;/p&gt;
&lt;p&gt;Anyway, I've caught up on the maintenance. Bootstrap 3 is now
&lt;a href="https://blog.getbootstrap.com/2019/07/24/lts-plan/"&gt;end-of-life&lt;/a&gt; so
there is a longer term issue of this theme still being stuck on it. If I
end up migrating to a new theme, it might be worth investigating using
&lt;a href="https://gohugo.io/"&gt;Hugo&lt;/a&gt; instead of Pelican at the same time. Hugo has
increased in popularity considerably since I started using Pelican.&lt;/p&gt;
&lt;h2 id="learning-from-other-projects"&gt;Learning from other projects&lt;/h2&gt;
&lt;p&gt;While spending a fair amount of time updating my theme and working with
the pelican-themes repository, I had chance to think about how the
pelican-themes repository is structured and lessons I've learned just
from having to work with it.&lt;/p&gt;
&lt;p&gt;Reading and understanding — or at least attempting to understand —
other people's source code is a way of picking up new ideas or concepts.
What might also be useful for learning is looking at projects you use as
a whole. How are they organised? How does that makes them easy or
difficult to work with?&lt;/p&gt;
&lt;p&gt;There is a caveat that even a public source code repository doesn't
necessarily yield all the decision making that went on to get it to such
a state. Some of that process may indeed be located with the source
repository, e.g. pull request comment threads or issues. Some of that
process may be separate to the source repository, but still public, e.g.
public mailing lists. Some of that process may be entirely private, e.g.
emails or private working documents. Some of it may be undocumented at
all and reside entirely within maintainers' heads.&lt;/p&gt;
&lt;h2 id="pelican-themes"&gt;pelican-themes&lt;/h2&gt;
&lt;h3 id="a-note"&gt;A note&lt;/h3&gt;
&lt;p&gt;The tone of this post shouldn't be considered as a ranting "why, oh why,
is this project not doing things the way I suggest?" post, but primarily
an exercise to just consider and note other ways it could have been
structured.&lt;/p&gt;
&lt;p&gt;It's an actively maintained project, and the maintainers are
volunteering considerable amounts of their own time doing just that.&lt;/p&gt;
&lt;p&gt;In many projects, whether by a single developer or a team, whether
commercial and proprietary or open source, it's also entirely believable
that there's no long-term maintenance plan from the outset. At the
start, it's not even known whether the project will have a long-term
future. The first commit of pelican-themes was in February 2011, making
it nine years old in 2020. Often, things do just get worked out along
the way.&lt;/p&gt;
&lt;h3 id="the-current-state"&gt;The current state&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/getpelican/pelican-themes/"&gt;pelican-themes&lt;/a&gt; is a
mixture of submodules and themes whose files exist entirely in the
repository. The included non-submodule themes sometimes (if not often or
even always) use the pelican-themes repository as their definitive home.
There are a few problems I've seen with this approach, exemplified by
the theme I am currently using my &lt;a href="https://github.com/StevenMaude/pelican-bootstrap3-sm"&gt;own
fork&lt;/a&gt; of,
pelican-bootstrap3.&lt;/p&gt;
&lt;p&gt;Back when pelican-bootstrap3 was still maintained by its original
developer, there were actually changes made to the version in
&lt;a href="https://github.com/getpelican/pelican-themes/commit/c817f12a9f5034a05abec4d2515adabd003f9ac0#diff-32c97fb49ece91afc9f43c8405423109"&gt;pelican-themes&lt;/a&gt;
that diverged from pelican-bootstrap3. pelican-bootstrap3 within
&lt;a href="https://github.com/getpelican/pelican-themes/commit/faa85d6112a759767a4f327e35a07fa55d9e747e#diff-0eb6cb930365747af1fe070650593b8e"&gt;pelican-themes&lt;/a&gt;
was then updated by simply a straight copy over of the then-current
version of the upstream theme, losing the changes made to the
pelican-themes version. Apart from losing work, this process can be
potentially confusing for users not directly following the development
process; you can have something that was fixed, then &lt;em&gt;unfixed&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Later, the official home of pelican-bootstrap3 became the pelican-themes
repository as the original developer &lt;a href="https://github.com/getpelican/pelican-themes/pull/383"&gt;no longer wanted to support
it&lt;/a&gt;. This had a
couple of consequences.&lt;/p&gt;
&lt;p&gt;First, it meant that the pelican-themes maintainers adopted the extra
maintenance work of a popular theme. Those maintainers may not even
&lt;em&gt;use&lt;/em&gt; the theme. This means that pull requests can languish, perhaps
because the maintainers don't feel that comfortable merging substantial
changes or don't have strong opinions on whether the proposed changes
are worthwhile. For pelican-bootstrap3, at the time of writing in
December 2019, there are several unmerged pull requests on
pelican-themes right now, even as &lt;a href="https://github.com/getpelican/pelican-themes/pull/414"&gt;far back as
2016&lt;/a&gt;.  Pull
requests left unloved and unmerged are a deterrent to other developers
who may be considering contributing other changes.&lt;/p&gt;
&lt;p&gt;Second, even if pull requests are regularly merged, there may be less of
a definitive direction taken than if maintainers have strong opinions
about where that subproject is headed. It can also lead to a lack of
quality assurance. For pelican-bootstrap3, an example is this
&lt;a href="https://github.com/getpelican/pelican-themes/pull/441"&gt;translation
feature&lt;/a&gt;. Such a
feature is one that's useful to lots of people, but unfortunately it
&lt;a href="https://github.com/getpelican/pelican-themes/issues/460"&gt;broke things&lt;/a&gt;
for users who didn't have the same plugin configuration as the original
author, affecting those users who did not intend to use the new feature.&lt;/p&gt;
&lt;h3 id="how-else-might-pelican-themes-be-managed"&gt;How else might pelican-themes be managed?&lt;/h3&gt;
&lt;p&gt;That's how things are structured in pelican-themes now. I did think
about other ways the repository might have been structured taking the
above features of how it works right now into account.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;pelican-themes could simply have been a list of links to themes,
  solely a reference resource.&lt;/p&gt;
&lt;p&gt;The advantage of this is no management on the part of the
pelican-themes maintainers is required except to add new themes or
remove themes that are no longer supported or unavailable. In many
cases, once a theme is added, that would be as much as is ever
required to be done for that theme. The big disadvantage is that you
can't just clone the entire repository and have all the themes ready
to use. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There could be a "more strongly" monorepo version of what there is
  now, that is, pelican-themes could be a pure monorepo with no
  submodules.  This now means that &lt;em&gt;every theme&lt;/em&gt; has to be
  hand-maintained by copying from the upstream version repository, if
  pelican-themes isn't the original repository for a theme. This
  doesn't, however, solve the problem of having a mixture of maintained
  and unmaintained themes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Going completely in the opposite direction is an option too: have
  every theme in pelican-themes included as a submodule. This has the
  advantage that pull requests into pelican-themes would be simple,
  simply reflecting a submodule update to the parent repository's
  current release.&lt;/p&gt;
&lt;p&gt;It would mean that every theme in pelican-themes where
pelican-themes is the definitive repository for that theme would
need to be moved to its own individual repository. However, breaking
out the themes in this way might make it easier for those themes to
be managed by specific maintainers for each theme only (provided
volunteers could be found), as opposed to the current agglomerate
repository maintained by developers who may not use most of the
themes present.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, the problems described above are easy to spot when using a
project that's been around a while, but perhaps subtle at a project's
inception. I think this is symptomatic of software development. It is
possible to build something that takes on a direction beyond that
originally envisaged, especially projects that have many contributors or
maintainers over time, each of which may have their own opinions as to
how such a project should be nurtured.&lt;/p&gt;</content><category term="2019"></category><category term="git"></category><category term="project management"></category></entry><entry><title>Stopping Fujitsu's Battery Charging Control Update Tool from crashing Windows 10</title><link href="https://www.stevenmaude.co.uk/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10" rel="alternate"></link><published>2018-12-10T21:16:00+00:00</published><updated>2018-12-10T21:16:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2018-12-10:/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10</id><summary type="html">&lt;p&gt;Preventing Fujitsu's Battery Charging Control Update from
making Windows 10 crash with a blue screen error caused by
tdklib64.sys.&lt;/p&gt;</summary><content type="html">&lt;p&gt;(NB: There's some background here just by way of introduction. If you encounter
this problem and don't want to endure my waffling, check the setting I &lt;a href="#the-fix"&gt;mention
below&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;My Fujitsu laptop currently dual boots Windows and Ubuntu, but it's rare that I
boot into Windows. Nonetheless, I had a little time spare last week, I figured
I might as well catch up on things everywhere and update the Windows install.&lt;/p&gt;
&lt;p&gt;Dropping into that did update Windows, and all well and good. Or so I thought.&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The problem&lt;/h2&gt;
&lt;p&gt;On restarting, I noticed there was a Fujitsu prompt that appeared, and was a
little unexpected. What I was being asked to install was the very precisely
named "Battery Charging Control Update Tool". It seems &lt;a href="https://www.fujitsu.com/global/about/resources/news/notices/2018/1031-01.html"&gt;Fujitsu have had some
issues with battery
quality&lt;/a&gt;,
leading to a potential fire risk; this tool is supposed to mitigate that.&lt;/p&gt;
&lt;p&gt;It seems that, at least for my model of laptop, the tool was attempting to
update the BIOS. And you can see how the &lt;a href="https://www.fujitsu.com/hk/support/products/computing/pc/ap/announcements/battery-control-update-tool.html"&gt;tool &lt;em&gt;should&lt;/em&gt; work on Fujitsu's Hong
Kong site&lt;/a&gt;
(and that page I could only find on the Hong Kong site for some reason).&lt;/p&gt;
&lt;p&gt;In the previous paragraph, I say &lt;em&gt;attempting&lt;/em&gt; to update the BIOS, because what
happened, after the initial preparation stage occurred with the "Continue to
update BIOS?" prompt, was that shortly after I clicked "Yes", I saw a lovely
Windows 10 blue screen which I think mentioned tdklib64.sys as the cause.&lt;/p&gt;
&lt;p&gt;There's nothing I could find relating to this failed BIOS update and these blue
screens — although there were mentions of BIOS update failures and tdklib64.sys
relating to other manufacturers machines. Finding nothing struck me as strange:
it's a fair bet that you'll find at least one person's already complained
loudly somewhere about a problem you've encountered too. &lt;/p&gt;
&lt;p&gt;Anyway, I rebooted and tried the update again. Same result. Blue screen.&lt;/p&gt;
&lt;p&gt;OK then. Third time lucky, maybe? No. Just the same (reliably) unreliable
result.&lt;/p&gt;
&lt;h2 id="the-fix"&gt;The fix&lt;/h2&gt;
&lt;p&gt;In a moment of fortunate (and rare) inspiration, I remembered that I'd turned on a
relatively new security feature — &lt;a href="https://support.microsoft.com/en-us/help/4096339/windows-10-device-protection-in-windows-defender-security-center"&gt;Memory
Integrity&lt;/a&gt;
— in Windows Defender Security Center as I'd spotted it as a setting I'd not
enabled already. Perhaps that was the culprit, especially as no-one else seemed
to have encounter this failure yet?&lt;/p&gt;
&lt;p&gt;Yes. Yes, it was.&lt;/p&gt;
&lt;p&gt;Disabling that again meant the update proceeded without a problem, and then I
just re-enabled the Memory Integrity setting once again after the BIOS update
completed. If you're having a similar problem, check this setting before
attempting to update. Maybe this tip helps you avoid the hour I spent figuring
this out.&lt;/p&gt;</content><category term="2018"></category><category term="fix"></category><category term="Windows"></category></entry><entry><title>git: nice and lease-y</title><link href="https://www.stevenmaude.co.uk/posts/git-nice-and-lease-y" rel="alternate"></link><published>2018-11-10T13:23:00+00:00</published><updated>2018-11-10T13:23:00+00:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2018-11-10:/posts/git-nice-and-lease-y</id><summary type="html">&lt;p&gt;Why generally using &lt;code&gt;git push --force-with-lease&lt;/code&gt; over &lt;code&gt;git push --force&lt;/code&gt; seems sensible.&lt;/p&gt;</summary><content type="html">&lt;h2 id="can-you-feel-the-force"&gt;Can you feel the &lt;code&gt;--force&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;Force pushing to a remote repository that others may be using should
always be done with care. Even if it's a completely private repository
that only you use, you maybe should double check your thinking before
going ahead.&lt;/p&gt;
&lt;p&gt;The double edged sword of a force push is that you're changing the state
of the remote repository history irrevocably.&lt;/p&gt;
&lt;p&gt;This can be good, for instance, if you're working on a non-master
repository branch that you "own" and have perhaps cleaned it up:
rebasing it, removing or squashing unneeded separate commits.&lt;/p&gt;
&lt;p&gt;It can be bad if you force push to master on a repository, and cause, at
minimum, considerable inconvenience for other developers. Those
developesr may now face working out what exactly has happened to the
repository, when they try and integrate their future changes, or may be
baffled that a previously existing commit has now mysteriously
disappeared.&lt;/p&gt;
&lt;p&gt;If you're working on a non-master development branch, you may be a
little bit more lax in how you force push. Certainly, the way I've used
branches with other people is that generally a branch is owned by one
particular person, and those are free to be amended by that branch owner
(usually the creator of that branch, although ownership may be passed
from person to person). Then, as the agreed owner of such a branch,
provided I know that I'm happy with the local changes, I can just force
push to that development branch.&lt;/p&gt;
&lt;p&gt;However, that's not always the case. Perhaps two people are working on
the same branch, maybe working on slightly different things, e.g. one
could be working on frontend changes for the site, while another works
on backend changes, but these changes are part of the same feature, and
therefore need to be part of the same branch. &lt;/p&gt;
&lt;h2 id="what-force-with-lease-offers-over-vanilla-force"&gt;What &lt;code&gt;--force-with-lease&lt;/code&gt; offers over vanilla &lt;code&gt;--force&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;What I discovered recently, although it's been around for ages, is that
git has another option for forcing push: &lt;code&gt;--force-with-lease&lt;/code&gt;. What this
option does is checks that the remote branch is still in the same state
it was when you last pulled it, and refuses to force push if not, i.e.
there have been no other changes to the branch in the intervening time.&lt;/p&gt;
&lt;p&gt;Of course, you can always still override this check by just using plain
old &lt;code&gt;--force&lt;/code&gt;. But &lt;code&gt;--force-with-lease&lt;/code&gt; at least gives you another
safety check before force pushing, just in case someone else has altered
the remote branch (giving you chance, for example, to pull that branch,
and rebase your changes on it), and avoiding any confusion between
developers, and potentially loss of work.&lt;/p&gt;
&lt;p&gt;Note though, &lt;a href="https://stackoverflow.com/a/43726130"&gt;as this answer
highlights&lt;/a&gt;, if you have an editor
or other scheduled task running &lt;code&gt;git fetch&lt;/code&gt; in the background,
&lt;code&gt;--force-with-lease&lt;/code&gt; won't offer any protection as the remote tracking
branches that would be stored locally are being periodically updated.&lt;/p&gt;</content><category term="2018"></category><category term="git"></category></entry><entry><title>A look at Pipenv</title><link href="https://www.stevenmaude.co.uk/posts/a-look-at-pipenv" rel="alternate"></link><published>2018-08-26T14:06:00+01:00</published><updated>2020-06-19T12:39:00+01:00</updated><author><name>Steven Maude</name></author><id>tag:www.stevenmaude.co.uk,2018-08-26:/posts/a-look-at-pipenv</id><summary type="html">&lt;p&gt;A quick look at Pipenv, a tool to manage Python packages.&lt;/p&gt;</summary><content type="html">&lt;div class="admonition article-edit"&gt;
&lt;p&gt;Edit 2020-06-19: These days, I just tend to use a manually created
virtualenv when working on Python, and a bash one-liner to activate
the appropriate virtualenv.&lt;/p&gt;
&lt;p&gt;As mentioned below, &lt;a href="https://github.com/python-poetry/poetry"&gt;poetry&lt;/a&gt; is
another option for managing dependencies and I've seen a lot of
positive things written about poetry since I originally wrote this
post.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a href="https://github.com/pypa/pipenv"&gt;Pipenv&lt;/a&gt;&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; is a tool that aims to
remove the hassle of using
&lt;a href="https://www.stevenmaude.co.uk/posts/explaining-python-virtualenv-in-under"&gt;virtualenvs&lt;/a&gt;
(Python runtime environments to keep separate Python setups for
different projects independent), and also help manage requirements. It
also aims to help provide deterministic builds of software.&lt;/p&gt;
&lt;p&gt;I had read about Pipenv previously, but couldn't ever really understand
how it worked from just reading about it. So I switched from my existing
virtualenv setup to this to try it out and figure out whether I can
replicate the same behaviour I had with virtualenv and
virtualenvwrapper.&lt;/p&gt;
&lt;h2 id="differences"&gt;Differences&lt;/h2&gt;
&lt;p&gt;The main difference of Pipenv to the way you might work with virtualenvs
— which, for me, is switch to a particular virtualenv for a project, then
run commands in the shell as normal, just with a self-contained Python
setup — is that Pipenv is more contextual.&lt;/p&gt;
&lt;p&gt;With Pipenv, you change to the appropriate project directory and then
run commands directly in that virtualenv by preceding them with &lt;code&gt;pipenv
run&lt;/code&gt;, e.g. &lt;code&gt;pipenv run myscript.py&lt;/code&gt;. Pipenv knows which virtualenv to
use based on the location you're in.&lt;/p&gt;
&lt;p&gt;(You can also get more virtualenv-like behaviour by going to a project
directory and doing &lt;code&gt;pipenv shell&lt;/code&gt; where it effectively activates the
virtualenv in the shell. However, one downside with this is that the
subshell command history there only exists within that subshell; it is
not stored in your main shell.)&lt;/p&gt;
&lt;p&gt;Another difference is that you would also tend to favour using &lt;code&gt;pipenv&lt;/code&gt;
over &lt;code&gt;pip&lt;/code&gt; for installing packages; &lt;code&gt;pipenv install &amp;lt;SOME_PACKAGE&amp;gt;&lt;/code&gt; also
adds packages to your project's
&lt;a href="https://github.com/pypa/pipfile"&gt;&lt;code&gt;Pipfile&lt;/code&gt;&lt;/a&gt;, which replaces the older
&lt;code&gt;requirements.txt&lt;/code&gt; file of specifying dependencies.&lt;/p&gt;
&lt;h2 id="usage"&gt;Usage&lt;/h2&gt;
&lt;p&gt;You can create a new Pipenv either implicitly by &lt;code&gt;pipenv install --dev&lt;/code&gt;
which installs dependencies for that project (including dev
dependencies) in a new virtualenv it creates, or more explicitly by:
&lt;code&gt;pipenv --two&lt;/code&gt; or &lt;code&gt;pipenv --three&lt;/code&gt; which gets you a new virtualenv with
that version of Python.&lt;/p&gt;
&lt;p&gt;You can also use &lt;code&gt;--python &amp;lt;VERSION_NUMBER&amp;gt;&lt;/code&gt; to use a specific point
release of Python of your choice you have installed, e.g. 3.7.
Furthermore, if you have pyenv installed, it will install the requested
version of Python for you, if that version is not installed already.&lt;/p&gt;
&lt;p&gt;There are two broad uses of virtualenvs I had:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;for Python development to avoid any clash of package versions, which
   is covered quite well by the default behaviour of Pipenv.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;for running standalone Python software I want to run, e.g.
   Docker Compose, but keeping their installations entirely independent
   of each other to avoid any conflicts. This isn't quite covered as
   well, because &lt;code&gt;pipenv run&lt;/code&gt; requires you to be in the directory or a
   subdirectory of that directory&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt; where you've already run Pipenv
   and a Pipfile exists, as that's how I assume it figures out which
   virtualenv to use. Otherwise, running Pipenv creates a new
   virtualenv! I think &lt;code&gt;pipenv shell&lt;/code&gt; works around this dropping you
   into a new subshell where the virtualenv is activated.&lt;/p&gt;
&lt;p&gt;Alternatively, in bash, you can do &lt;code&gt;source $(pipenv
--venv)/bin/activate&lt;/code&gt; in the directory with the virtualenv, to work
at a slightly lower level, with the virtualenv directly, without
&lt;code&gt;pipenv shell&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These use cases are both handled reasonably well by Pipenv.&lt;/p&gt;
&lt;h2 id="summary"&gt;Summary&lt;/h2&gt;
&lt;p&gt;Pipenv seems to work well enough if you want to simplify using
virtualenvs and management of dependencies. I think there are two groups
of users that it might particularly suit: those who are newer to
managing Python packaging and virtualenvs — as a means of abstracting
away virtualenv management — and those who are more experienced but want
to have a tool that integrates other features, e.g. checking for
dependency vulnerabilities via &lt;code&gt;pipenv check&lt;/code&gt; (which is provided by the
&lt;a href="https://github.com/pyupio/safety"&gt;safety&lt;/a&gt; package).&lt;/p&gt;
&lt;p&gt;However, if you read around, there is still some contention about Pipenv
being &lt;a href="https://packaging.python.org/guides/tool-recommendations/"&gt;recommended by the
PyPA&lt;/a&gt;.
Certainly, there are a considerable number of small issues that remove
some of Pipenv's sheen; &lt;a href="https://github.com/pypa/pipenv/issues/2753"&gt;I encountered a small one already
reported&lt;/a&gt; within just a
short time of using Pipenv. There are also several issues relating to
dependency resolution, which seem a little more critical seeing as
dependency management is one of the tool's core goals.&lt;/p&gt;
&lt;p&gt;Pipenv then is no panacea for Python's still byzantine dependency
management. &lt;a href="https://www.python.org/dev/peps/pep-0020/"&gt;PEP 20&lt;/a&gt;'s call
for "one obvious way to do it" is not yet fully heeded. But maybe it's a
step roughly in the right direction, even if there's still some
meandering to do before there's a really simple and transparent workflow
for Python. (However, I don't think it's uncommon for dependency
management being tricky to get right either; Go has been around for a
decade and is only just &lt;a href="https://blog.golang.org/versioning-proposal"&gt;getting
there&lt;/a&gt;. Python's had
considerably longer than Go to get it right though.)&lt;/p&gt;
&lt;p&gt;Finally, it is worth noting that there are other alternatives too.
&lt;a href="https://github.com/python-poetry/poetry"&gt;Poetry&lt;/a&gt; is a newer, and perhaps
less well known, tool whose goals intersect with those of Pipenv. It
fixes some of the existing issues of dependency resolution that
pip-tools has (pip-tools is the underlying package that pipenv actually
uses for this task), and therefore may be another useful contender to
keep in mind.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Confusingly, Pipenv's name is capitalised, while pip's is not.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Caveat: it only looks a certain number of subdirectories deep by
default, though &lt;a href="https://github.com/pypa/pipenv/issues/1634"&gt;this number is configurable&lt;/a&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="2018"></category><category term="Pipenv"></category><category term="Python"></category></entry></feed>